{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ex.\r\n",
      "# Now in use: ccc\r\n",
      "# Start time: 03/31 12:00\r\n",
      "# Estimated end time(option): 03/31 18:00\r\n",
      "\r\n",
      "Now in use: \r\n",
      "Start time:  \r\n",
      "Estimated end time(option):  \r\n",
      "\r\n",
      "Next one who want to use: \r\n",
      "Estimated start time:\r\n",
      "Estimated end time(option): \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ~/user_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 25 18:20:30 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4090         On | 00000000:01:00.0 Off |                  Off |\r\n",
      "|  0%   45C    P8               38W / 480W|    556MiB / 24564MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      1158      G   /usr/lib/xorg/Xorg                            4MiB |\r\n",
      "|    0   N/A  N/A   2921994      C   .../anaconda3/envs/sheng-ru/bin/python      548MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_array_create(dirname, dir_list, time_seq):\n",
    "    \n",
    "    columns = ['RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "               'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2']\n",
    "    \n",
    "    def reamin_HO_time(y_train):\n",
    "        def f(L):    \n",
    "            for i, e in enumerate(L):\n",
    "                if e: return i+1\n",
    "            return 0\n",
    "\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            a1_out = []\n",
    "            for a1 in a2:\n",
    "                a1_out.append(a1.any())\n",
    "      \n",
    "            out.append(f(a1_out))\n",
    "        return out\n",
    "    \n",
    "    def HO(y_train):\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            if sum(a2.reshape(-1)) == 0: ho = 0\n",
    "            elif sum(a2.reshape(-1)) > 0: ho = 1\n",
    "            out.append(ho)\n",
    "        return out\n",
    "\n",
    "    split_time = []\n",
    "    for i, f in enumerate(tqdm(dir_list)):\n",
    "    \n",
    "        f = os.path.join(dirname, f)\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # preprocess data with ffill method\n",
    "        del df['Timestamp'], df['lat'], df['long'], df['gpsspeed']\n",
    "        # df[columns] = df[columns].replace(0, np.nan)\n",
    "        # df[columns] = df[columns].fillna(method='ffill')\n",
    "        # df.dropna(inplace=True)\n",
    "        \n",
    "        df.replace(np.nan,0,inplace=True); df.replace('-',0,inplace=True)\n",
    "        \n",
    "        X = df[features]\n",
    "        Y = df[target]\n",
    "\n",
    "        Xt_list = []\n",
    "        Yt_list = []\n",
    "\n",
    "        for j in range(time_seq):\n",
    "            X_t = X.shift(periods=-j)\n",
    "            Xt_list.append(X_t)\n",
    "    \n",
    "        for j in range(time_seq,time_seq+predict_t):\n",
    "            Y_t = Y.shift(periods=-(j))\n",
    "            Yt_list.append(Y_t)\n",
    "\n",
    "        # YY = Y.shift(periods=-(0))\n",
    "\n",
    "        X_ts = np.array(Xt_list); X_ts = np.transpose(X_ts, (1,0,2)); X_ts = X_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        Y_ts = np.array(Yt_list); Y_ts = np.transpose(Y_ts, (1,0,2)); Y_ts = Y_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        split_time.append(len(X_ts))\n",
    "\n",
    "        if i == 0:\n",
    "            X_final = X_ts\n",
    "            Y_final = Y_ts\n",
    "        else:\n",
    "            X_final = np.concatenate((X_final,X_ts), axis=0)\n",
    "            Y_final = np.concatenate((Y_final,Y_ts), axis=0)\n",
    "\n",
    "    split_time = [(sum(split_time[:i]), sum(split_time[:i])+x) for i, x in enumerate(split_time)]\n",
    "    \n",
    "    return X_final, np.array(HO(Y_final)), np.array(reamin_HO_time(Y_final)), split_time # forecast HO\n",
    "\n",
    "class RNN_Dataset_simple(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset take all csv file specified in dir_list in directory dirname.\n",
    "    Transfer csvs to (features, label) pair\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.inputs = torch.FloatTensor(X)\n",
    "        self.labels = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_file(file, dates):\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in file: return True \n",
    "    return False\n",
    "\n",
    "def train_valid_split(L, valid_size=0.2):\n",
    "    \n",
    "    length = len(L)\n",
    "    v_num = int(length*valid_size)\n",
    "    v_files = random.sample(L, v_num)\n",
    "    t_files = list(set(L) - set(v_files))\n",
    "    \n",
    "    return t_files, v_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n",
    "    \n",
    "    print(f\"Random seed set as {seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time sequence length and prediction time length\n",
    "seed = 55688\n",
    "time_seq = 20\n",
    "predict_t = 10\n",
    "valid_ratio = 0.2\n",
    "task = 'classification'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "Loading training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0672736c6a7f410bb4c7d7cadf97cd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8fcc114cfa452aa7432da9b00bb547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup seed\n",
    "set_seed(seed)\n",
    "\n",
    "# Get GPU\n",
    "device_count = torch.cuda.device_count()\n",
    "num_of_gpus = device_count\n",
    "\n",
    "for i in range(device_count):\n",
    "    print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "    gpu_id = i\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Save best model to \n",
    "save_path = \"/home/wmnlab/Documents/sheng-ru/YU/model\"\n",
    "\n",
    "# Define DataSet\n",
    "dirname = \"/home/wmnlab/Documents/sheng-ru/data/single\"\n",
    "# dirname = \"/home/wmnlab/Documents/sheng-ru/data/single0.5\"\n",
    "dir_list = os.listdir(dirname)\n",
    "dir_list = [f for f in dir_list if ( f.endswith('.csv') and (not 'sm' in f) ) ]\n",
    "\n",
    "train_dates = ['03-26', '04-01']\n",
    "test_dates = ['04-10']\n",
    "    \n",
    "# train_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, train_dates) )]\n",
    "# test_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, test_dates) )]\n",
    "\n",
    "train_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio)\n",
    "train_dir_list += [f for f in os.listdir(dirname) if 'sm' in f]\n",
    "\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "#         'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2' ]\n",
    "features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "        'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1','nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2']\n",
    "\n",
    "num_of_features = len(features)\n",
    "\n",
    "# target = ['LTE_HO', 'MN_HO'] # For eNB HO.\n",
    "# target = ['eNB_to_ENDC'] # Setup gNB\n",
    "target = ['gNB_Rel', 'gNB_HO'] # For gNB HO.\n",
    "# target = ['RLF'] # For RLF\n",
    "# target = ['SCG_RLF'] # For scg failure\n",
    "# target = ['dl-loss'] # For DL loss\n",
    "# target = ['ul-loss'] # For UL loss\n",
    "\n",
    "# Data\n",
    "print('Loading training data...')\n",
    "X_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq)\n",
    "\n",
    "train_dataset = RNN_Dataset_simple(X_train, y_train1)\n",
    "train_dataloader1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_train2 > 0\n",
    "X_train_fore = X_train[cond]\n",
    "y_train2_fore = y_train2[cond]\n",
    "train_dataset = RNN_Dataset_simple(X_train_fore, y_train2_fore)\n",
    "train_dataloader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Loading testing data...')\n",
    "X_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq)\n",
    "\n",
    "test_dataset = RNN_Dataset_simple(X_test, y_test1)\n",
    "test_dataloader1 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_test2 > 0\n",
    "X_test_fore = X_test[cond]\n",
    "y_test2_fore = y_test2[cond]\n",
    "test_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\n",
    "test_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = next(iter(train_dataloader1))\n",
    "input_dim, out_dim = a.shape[2], 1\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Cls(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "        \n",
    "        out = torch.sigmoid(out) # Binary Classifier\n",
    "\n",
    "        return out\n",
    "\n",
    "class RNN_Fst(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "# Define model and optimizer\n",
    "\n",
    "classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[600, 1000], gamma=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        classifier.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = classifier(features)\n",
    "            \n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, auc: {roc_auc}, aucpr: {aucpr}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        classifier.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = classifier(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, auc: {roc_auc}, aucpr: {aucpr}')\n",
    "        \n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(classifier.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.6813831408834969, auc: 0.540311232500345, aucpr: 0.45126662210363444; Epoch 1 valid loss: 0.6758134104624516, auc: 0.5944508875533029, aucpr: 0.5243918562070217\n",
      "Best model found! Loss: 0.6758134104624516\n",
      "Epoch 2 train loss: 0.6723435944842957, auc: 0.5744849342001317, aucpr: 0.4794539698510676; Epoch 2 valid loss: 0.6793891326093874, auc: 0.5765839792946938, aucpr: 0.5232915959838402\n",
      "Epoch 3 train loss: 0.6681703467638802, auc: 0.5912832664810548, aucpr: 0.5079660074770878; Epoch 3 valid loss: 0.6711584659559386, auc: 0.5978802883671762, aucpr: 0.5417712594802934\n",
      "Best model found! Loss: 0.6711584659559386\n",
      "Epoch 4 train loss: 0.6585163461224389, auc: 0.6215146791357409, aucpr: 0.5421490968210577; Epoch 4 valid loss: 0.6577520056807694, auc: 0.6392133909782334, aucpr: 0.5773712285126873\n",
      "Best model found! Loss: 0.6577520056807694\n",
      "Epoch 5 train loss: 0.6487601145880465, auc: 0.6462109937180963, aucpr: 0.5631774588150779; Epoch 5 valid loss: 0.6445185972236785, auc: 0.6747141151063861, aucpr: 0.6069726238597947\n",
      "Best model found! Loss: 0.6445185972236785\n",
      "Epoch 6 train loss: 0.638358089146931, auc: 0.6681429007061548, aucpr: 0.5819480268331414; Epoch 6 valid loss: 0.6394099861132998, auc: 0.689151728150211, aucpr: 0.6228807160490646\n",
      "Best model found! Loss: 0.6394099861132998\n",
      "Epoch 7 train loss: 0.6284936218530949, auc: 0.685687468347387, aucpr: 0.6001819180769673; Epoch 7 valid loss: 0.6463090598332781, auc: 0.6733244662951645, aucpr: 0.6070538763541893\n",
      "Epoch 8 train loss: 0.6199237359803044, auc: 0.6998925267547947, aucpr: 0.6155746555347127; Epoch 8 valid loss: 0.6419175552094684, auc: 0.6720755810695471, aucpr: 0.5952564687830427\n",
      "Epoch 9 train loss: 0.6152511122586508, auc: 0.7063210383723041, aucpr: 0.6212238849789189; Epoch 9 valid loss: 0.639125325912688, auc: 0.6812618218180108, aucpr: 0.6297905356731315\n",
      "Best model found! Loss: 0.639125325912688\n",
      "Epoch 10 train loss: 0.6074709115222305, auc: 0.7168609909795429, aucpr: 0.6344342282446878; Epoch 10 valid loss: 0.6251349660153148, auc: 0.7089654260134767, aucpr: 0.6397915300473611\n",
      "Best model found! Loss: 0.6251349660153148\n",
      "Epoch 11 train loss: 0.6038072452896592, auc: 0.7217182630799133, aucpr: 0.643205205505478; Epoch 11 valid loss: 0.622860260965193, auc: 0.7109584680191047, aucpr: 0.653367107918621\n",
      "Best model found! Loss: 0.622860260965193\n",
      "Epoch 12 train loss: 0.599046891098373, auc: 0.7275407472776985, aucpr: 0.6493391218725946; Epoch 12 valid loss: 0.6095720574009318, auc: 0.7263839253405994, aucpr: 0.6700821664741328\n",
      "Best model found! Loss: 0.6095720574009318\n",
      "Epoch 13 train loss: 0.5928415783444876, auc: 0.7354934176176198, aucpr: 0.6578400228666506; Epoch 13 valid loss: 0.6205682143443773, auc: 0.711867992972451, aucpr: 0.655349434441081\n",
      "Epoch 14 train loss: 0.5899916180431793, auc: 0.7385786796710383, aucpr: 0.6665866291972771; Epoch 14 valid loss: 0.6328560215961031, auc: 0.6962611868154891, aucpr: 0.643387407474139\n",
      "Epoch 15 train loss: 0.58692449662899, auc: 0.7431384716542311, aucpr: 0.6706585320882967; Epoch 15 valid loss: 0.6418617637280155, auc: 0.6847673595750354, aucpr: 0.6322165372899422\n",
      "Epoch 16 train loss: 0.5835019022681612, auc: 0.7457277670032311, aucpr: 0.6731030369987041; Epoch 16 valid loss: 0.6732834221936074, auc: 0.6615841003816492, aucpr: 0.6146957430262695\n",
      "Epoch 17 train loss: 0.581083138034203, auc: 0.7505722079196784, aucpr: 0.6792945326218767; Epoch 17 valid loss: 0.6251669161710418, auc: 0.7153210479822549, aucpr: 0.6621522534587221\n",
      "Epoch 18 train loss: 0.5766612298780488, auc: 0.7549631841524111, aucpr: 0.686225412799788; Epoch 18 valid loss: 0.6569266196559457, auc: 0.6900526643776539, aucpr: 0.6427757558640016\n",
      "Epoch 19 train loss: 0.574690405088673, auc: 0.7562180363734323, aucpr: 0.6831881520063482; Epoch 19 valid loss: 0.6239939288181418, auc: 0.7165238920050789, aucpr: 0.6662698262621622\n",
      "Epoch 20 train loss: 0.5764958608824787, auc: 0.7533586859125443, aucpr: 0.6870171391428874; Epoch 20 valid loss: 0.6320393596376691, auc: 0.7139442118604955, aucpr: 0.6603008778322597\n",
      "Epoch 21 train loss: 0.5688244724180165, auc: 0.7627991025602914, aucpr: 0.6961253249283621; Epoch 21 valid loss: 0.6262111597764892, auc: 0.7093282292835003, aucpr: 0.6558133321258595\n",
      "Epoch 22 train loss: 0.5640741667343406, auc: 0.7676467325574571, aucpr: 0.7030913161474464; Epoch 22 valid loss: 0.6065397051577809, auc: 0.7247211972852868, aucpr: 0.6729256866316332\n",
      "Best model found! Loss: 0.6065397051577809\n",
      "Epoch 23 train loss: 0.5664935605685072, auc: 0.7646205072889618, aucpr: 0.6968505368369546; Epoch 23 valid loss: 0.6207333620825485, auc: 0.7124062667286557, aucpr: 0.6609950349412667\n",
      "Epoch 24 train loss: 0.5552788102164139, auc: 0.7760883612464327, aucpr: 0.7117051510205685; Epoch 24 valid loss: 0.6244031844832817, auc: 0.7135693350946322, aucpr: 0.6577977188535815\n",
      "Epoch 25 train loss: 0.5574187474259161, auc: 0.7745273123598261, aucpr: 0.7083220363035148; Epoch 25 valid loss: 0.6278292861681024, auc: 0.7016832776380271, aucpr: 0.6480562000694774\n",
      "Epoch 26 train loss: 0.5529164425201228, auc: 0.7789535100358995, aucpr: 0.7154904029072844; Epoch 26 valid loss: 0.6297844409660882, auc: 0.7302278025787016, aucpr: 0.672782263381305\n",
      "Epoch 27 train loss: 0.5507685592187327, auc: 0.7806518010044121, aucpr: 0.7182531751065819; Epoch 27 valid loss: 0.6119148305484227, auc: 0.727474693530324, aucpr: 0.6762072227188032\n",
      "Epoch 28 train loss: 0.553916643692825, auc: 0.7775403117224999, aucpr: 0.7151439276917432; Epoch 28 valid loss: 0.604037774319784, auc: 0.7282561971881356, aucpr: 0.6788995601694081\n",
      "Best model found! Loss: 0.604037774319784\n",
      "Epoch 29 train loss: 0.5427555747877754, auc: 0.7888506784479993, aucpr: 0.7292401973551076; Epoch 29 valid loss: 0.6246408339308089, auc: 0.729715822995652, aucpr: 0.6729512228065866\n",
      "Epoch 30 train loss: 0.5429121650706186, auc: 0.7879632007035088, aucpr: 0.7277961087673235; Epoch 30 valid loss: 0.6051718207905773, auc: 0.7360665522066409, aucpr: 0.687948079036105\n",
      "Epoch 31 train loss: 0.533010349085233, auc: 0.7973418270408129, aucpr: 0.7404354004179747; Epoch 31 valid loss: 0.6121005744618528, auc: 0.7363186876314181, aucpr: 0.678067886688797\n",
      "Epoch 32 train loss: 0.5472216242209114, auc: 0.7822890938284986, aucpr: 0.7215630964676283; Epoch 32 valid loss: 0.5999105294712451, auc: 0.7470242176726694, aucpr: 0.7003392774808375\n",
      "Best model found! Loss: 0.5999105294712451\n",
      "Epoch 33 train loss: 0.5260739329460975, auc: 0.8030240352425133, aucpr: 0.7456675001856364; Epoch 33 valid loss: 0.6122175778473625, auc: 0.7344232191840036, aucpr: 0.6817540458449294\n",
      "Epoch 34 train loss: 0.5270681110965157, auc: 0.8031347792350365, aucpr: 0.7479536296948724; Epoch 34 valid loss: 0.6117522786958378, auc: 0.7345786610429755, aucpr: 0.6824122556761445\n",
      "Epoch 35 train loss: 0.5295152758070499, auc: 0.8001057664132716, aucpr: 0.7452909372470913; Epoch 35 valid loss: 0.6288138136205053, auc: 0.7224030508843994, aucpr: 0.6715075849768175\n",
      "Epoch 36 train loss: 0.5204392825364821, auc: 0.8092253408518872, aucpr: 0.7549851691126739; Epoch 36 valid loss: 0.640538582922656, auc: 0.7187969123952773, aucpr: 0.6635533762532309\n",
      "Epoch 37 train loss: 0.5302870295877229, auc: 0.8000194053451988, aucpr: 0.7437786785782848; Epoch 37 valid loss: 0.611334650440752, auc: 0.7382066937432117, aucpr: 0.6867741071459401\n",
      "Epoch 38 train loss: 0.5288628565430531, auc: 0.8011429213405823, aucpr: 0.7456392163499912; Epoch 38 valid loss: 0.6100719386805632, auc: 0.7408387862356234, aucpr: 0.6859450790044481\n",
      "Epoch 39 train loss: 0.5216138900129216, auc: 0.808522907372327, aucpr: 0.7552867167943047; Epoch 39 valid loss: 0.6227833020086048, auc: 0.725610799210513, aucpr: 0.6755911073649529\n",
      "Epoch 40 train loss: 0.5245172021023061, auc: 0.8046843349088455, aucpr: 0.7511444031892454; Epoch 40 valid loss: 0.6251114968429593, auc: 0.7166719067281262, aucpr: 0.6687260937788144\n",
      "Epoch 41 train loss: 0.5190275287890882, auc: 0.810483036431688, aucpr: 0.7580716597129336; Epoch 41 valid loss: 0.6892977505922318, auc: 0.6435234881941627, aucpr: 0.6032441341510627\n",
      "Epoch 42 train loss: 0.5346858310678545, auc: 0.796234219699051, aucpr: 0.7417162413110925; Epoch 42 valid loss: 0.6231346067917698, auc: 0.7305393902905719, aucpr: 0.6849455517124731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 train loss: 0.5138779939140764, auc: 0.8143803816247537, aucpr: 0.7646529254552209; Epoch 43 valid loss: 0.6318187500749316, auc: 0.7261280587480115, aucpr: 0.6755460985714097\n",
      "Epoch 44 train loss: 0.5039959560916701, auc: 0.8224901648913981, aucpr: 0.7728676538115973; Epoch 44 valid loss: 0.6338017084987975, auc: 0.725199349960168, aucpr: 0.6794148916323439\n",
      "Epoch 45 train loss: 0.5132540153950133, auc: 0.815416276164209, aucpr: 0.7636638665014027; Epoch 45 valid loss: 0.6242764888472166, auc: 0.7228305159965723, aucpr: 0.667346481605277\n",
      "Epoch 46 train loss: 0.5140687792379682, auc: 0.8142087442932742, aucpr: 0.7618006004603123; Epoch 46 valid loss: 0.6347142567529398, auc: 0.7229542077294591, aucpr: 0.6728092827885861\n",
      "Epoch 47 train loss: 0.5126304473423396, auc: 0.8165156800960313, aucpr: 0.7667331281779444; Epoch 47 valid loss: 0.7047964091091847, auc: 0.6433022581027589, aucpr: 0.5921348755005199\n",
      "Epoch 48 train loss: 0.5182841822788009, auc: 0.8102650533048785, aucpr: 0.761621777385352; Epoch 48 valid loss: 0.6355711681564816, auc: 0.7205803754089852, aucpr: 0.6662671918684879\n",
      "Epoch 49 train loss: 0.5074314556886695, auc: 0.8200489732820642, aucpr: 0.767170165063078; Epoch 49 valid loss: 0.6310903779661455, auc: 0.7276684678585941, aucpr: 0.6851010446380227\n",
      "Epoch 50 train loss: 0.4959159612303414, auc: 0.8296220886265541, aucpr: 0.7825882695848536; Epoch 50 valid loss: 0.6727944033361283, auc: 0.6871087025758011, aucpr: 0.6431793349526023\n",
      "Epoch 51 train loss: 0.489560286059951, auc: 0.8347324169360847, aucpr: 0.7909363683294807; Epoch 51 valid loss: 0.6565481618735469, auc: 0.7188948379504541, aucpr: 0.6803338338151077\n",
      "Epoch 52 train loss: 0.4841090702843213, auc: 0.8391130446495255, aucpr: 0.7953753665133613; Epoch 52 valid loss: 0.6447102717523064, auc: 0.7225541631807233, aucpr: 0.6761666481268289\n",
      "Epoch 53 train loss: 0.49688048650296046, auc: 0.8279628587926782, aucpr: 0.7813965711839654; Epoch 53 valid loss: 0.6431483196900735, auc: 0.7290208401914076, aucpr: 0.6797715868775752\n",
      "Epoch 54 train loss: 0.49160498644599826, auc: 0.8328737232379914, aucpr: 0.7884456521519547; Epoch 54 valid loss: 0.6392541831523618, auc: 0.7243654691542245, aucpr: 0.6789999173639085\n",
      "Epoch 55 train loss: 0.4748221756452056, auc: 0.8458160458752868, aucpr: 0.8037830286630006; Epoch 55 valid loss: 0.6447843422393212, auc: 0.7209879174930383, aucpr: 0.6674658546119248\n",
      "Epoch 56 train loss: 0.48860413994575513, auc: 0.8340008679526216, aucpr: 0.7891074986357649; Epoch 56 valid loss: 0.636796207550694, auc: 0.7292468926412079, aucpr: 0.6751179344869189\n",
      "Epoch 57 train loss: 0.4686426312334276, auc: 0.8501222957330509, aucpr: 0.8090761571279783; Epoch 57 valid loss: 0.6498851188998512, auc: 0.725057107987318, aucpr: 0.6756233534922362\n",
      "Epoch 58 train loss: 0.48048471541449267, auc: 0.8404292638757815, aucpr: 0.7977007882170082; Epoch 58 valid loss: 0.6350689709202206, auc: 0.7377761662574586, aucpr: 0.6869191724201398\n",
      "Epoch 59 train loss: 0.47389901368659604, auc: 0.846067011419495, aucpr: 0.8045782449041446; Epoch 59 valid loss: 0.6582627822762402, auc: 0.7287583560559105, aucpr: 0.6801352488731071\n",
      "Epoch 60 train loss: 0.48403758908491995, auc: 0.8386378512058509, aucpr: 0.795234312929608; Epoch 60 valid loss: 0.635648217693973, auc: 0.7265600646210105, aucpr: 0.6711545686385012\n",
      "Epoch 61 train loss: 0.47561582027450755, auc: 0.8453484555920443, aucpr: 0.8031317045255709; Epoch 61 valid loss: 0.6478007252234183, auc: 0.7353155314858468, aucpr: 0.6848056868806849\n",
      "Epoch 62 train loss: 0.45775765460061446, auc: 0.8580381181031876, aucpr: 0.8213916734146104; Epoch 62 valid loss: 0.6765520035704019, auc: 0.7260858191124214, aucpr: 0.6789721710602743\n",
      "Epoch 63 train loss: 0.4578890536083516, auc: 0.8573846805001176, aucpr: 0.8192454617457888; Epoch 63 valid loss: 0.6615573361265559, auc: 0.7149144562900027, aucpr: 0.6627363534509975\n",
      "Epoch 64 train loss: 0.46120285708380326, auc: 0.8550663100122924, aucpr: 0.8170215904650386; Epoch 64 valid loss: 0.6300891758795796, auc: 0.7418622526059744, aucpr: 0.6975267395153433\n",
      "Epoch 65 train loss: 0.4484494725253024, auc: 0.86363535493012, aucpr: 0.8298553628801505; Epoch 65 valid loss: 0.6518720219209164, auc: 0.7384709730632212, aucpr: 0.6862748539684862\n",
      "Epoch 66 train loss: 0.4629821999907543, auc: 0.854470987722679, aucpr: 0.8139489741057759; Epoch 66 valid loss: 0.6657652372353468, auc: 0.7176714725046724, aucpr: 0.6660296479299723\n",
      "Epoch 67 train loss: 0.4621476621869229, auc: 0.8554780131191404, aucpr: 0.8178100326966402; Epoch 67 valid loss: 0.6747055198652905, auc: 0.7240895035350351, aucpr: 0.6782777447979481\n",
      "Epoch 68 train loss: 0.4766569872556714, auc: 0.8442127671899753, aucpr: 0.8027915007972688; Epoch 68 valid loss: 0.6341777132714496, auc: 0.7387598921706583, aucpr: 0.6856633685173157\n",
      "Epoch 69 train loss: 0.44989101059464115, auc: 0.8633302539580807, aucpr: 0.8258355640694939; Epoch 69 valid loss: 0.6653919327738029, auc: 0.7225390273113036, aucpr: 0.6797623402696671\n",
      "Epoch 70 train loss: 0.445284154793689, auc: 0.8665965341130643, aucpr: 0.8297376249208415; Epoch 70 valid loss: 0.6781587224500645, auc: 0.7313562696431906, aucpr: 0.6785052292313369\n",
      "Epoch 71 train loss: 0.4539544760148809, auc: 0.8603989062228968, aucpr: 0.8246368360314612; Epoch 71 valid loss: 0.6675563472471818, auc: 0.7322398524203452, aucpr: 0.6759933058614673\n",
      "Epoch 72 train loss: 0.45653501666565505, auc: 0.8587291550087596, aucpr: 0.8210036711645925; Epoch 72 valid loss: 0.6690785947545361, auc: 0.7210411394338819, aucpr: 0.6797628421436784\n",
      "Epoch 73 train loss: 0.4315717853559604, auc: 0.8749670938047608, aucpr: 0.8423636696530568; Epoch 73 valid loss: 0.6645550535475256, auc: 0.724841263449452, aucpr: 0.6725240201970226\n",
      "Epoch 74 train loss: 0.44775109913609273, auc: 0.8641803855665282, aucpr: 0.8289126687166597; Epoch 74 valid loss: 0.6887894386754316, auc: 0.7200235866125135, aucpr: 0.6742235973242934\n",
      "Epoch 75 train loss: 0.446577590351892, auc: 0.8653349416753477, aucpr: 0.8300606465505168; Epoch 75 valid loss: 0.674354064098301, auc: 0.7254562725436455, aucpr: 0.6725265901312495\n",
      "Epoch 76 train loss: 0.4384780073658395, auc: 0.8705442036259752, aucpr: 0.8362948368371854; Epoch 76 valid loss: 0.6706633867868105, auc: 0.7255917561748011, aucpr: 0.6723248592223108\n",
      "Epoch 77 train loss: 0.45295893543566756, auc: 0.8614214156066614, aucpr: 0.824076333281324; Epoch 77 valid loss: 0.714881093100402, auc: 0.7040294781968858, aucpr: 0.662009383897857\n",
      "Epoch 78 train loss: 0.4380655370485022, auc: 0.8707667369434979, aucpr: 0.8359671548493851; Epoch 78 valid loss: 0.6669564298667139, auc: 0.7324863558937105, aucpr: 0.6855825079813511\n",
      "Epoch 79 train loss: 0.43537316500792983, auc: 0.8725194954506805, aucpr: 0.8378652615026532; Epoch 79 valid loss: 0.6877154729039479, auc: 0.7187148267034471, aucpr: 0.6677843827675476\n",
      "Epoch 80 train loss: 0.4325944141509407, auc: 0.8744431032066553, aucpr: 0.8404521153118858; Epoch 80 valid loss: 0.6726083897930735, auc: 0.7354142314343426, aucpr: 0.6811677784482382\n",
      "Epoch 81 train loss: 0.4404791589929398, auc: 0.8690076356637387, aucpr: 0.8359858635514203; Epoch 81 valid loss: 0.6956121454860608, auc: 0.7124392488441122, aucpr: 0.66257464048597\n",
      "Epoch 82 train loss: 0.45323760132516816, auc: 0.8606491571843577, aucpr: 0.8252011465953266; Epoch 82 valid loss: 0.6887295137183005, auc: 0.7237351129924332, aucpr: 0.6699294462126169\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.5999105294712451, roc_auc 0.7470242176726694, aucpr 0.7003392774808375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5999105294712451,\n",
       " 0.7470242176726694,\n",
       " 0.7003392774808375,\n",
       " 0.6612184249628529,\n",
       " 0.5318195398864655,\n",
       " 0.5895015731081304)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def test(test_dataloader):\n",
    "    best_model = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        threshold = 0.5\n",
    "        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n",
    "        \n",
    "        return valid_loss, roc_auc, aucpr, p, r, f1\n",
    "        \n",
    "test(test_dataloader1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import itertools\n",
    "\n",
    "n_epochs = 600\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "hidden_dims = [32, 64, 128]\n",
    "num_layers = [1, 2]\n",
    "dropout = 0\n",
    "\n",
    "early_stopping_patience = 50\n",
    "rnn = 'GRU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning_rate = 0.1, hidden_dim = 128, num_layer = 2.\n",
      "valid loss 1.097691842581795, roc_auc 0.4998190383611922, aucpr 0.45540833132492897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmnlab/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "f_out = 'lte_ho_cls_rnn.csv'\n",
    "f_out = open(f_out, 'w')\n",
    "cols_out = ['lr','hidden_dim','num_layer', 'valid_loss','auc','aucpr', 'p', 'r', 'f1']\n",
    "f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "for lr, hidden_dim, num_layer in itertools.product(lrs, hidden_dims, num_layers):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # For record loss\n",
    "    train_losses_for_epochs = []\n",
    "    validation_losses_for_epochs = []\n",
    "    valid_losses_for_epochs = []\n",
    "\n",
    "    # Save best model to ... \n",
    "    best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "    \n",
    "    train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f'For learning_rate = {lr}, hidden_dim = {hidden_dim}, num_layer = {num_layer}.')\n",
    "    valid_loss, roc_auc, aucpr, p, r, f1 = test(test_dataloader1)\n",
    "    \n",
    "    cols_out = [lr, hidden_dim, num_layer, valid_loss, roc_auc, aucpr, p, r, f1]\n",
    "    cols_out = [str(n) for n in cols_out]\n",
    "    f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "forecaster = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(forecaster.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fst(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    def rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "    def mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        forecaster.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = forecaster(features)\n",
    "            \n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "        \n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, rmse: {rmse_error}, mae: {mae_error}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        forecaster.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = forecaster(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, rmse: {rmse_error}, mae: {mae_error}')\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(forecaster.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_fst_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 7.894060632015797, rmse: 2.809724807739258, mae: 2.404712677001953; Epoch 1 valid loss: 7.177412469046456, rmse: 2.678077220916748, mae: 2.3107211589813232\n",
      "Best model found! Loss: 7.177412469046456\n",
      "Epoch 2 train loss: 7.04768890107854, rmse: 2.654772996902466, mae: 2.2468719482421875; Epoch 2 valid loss: 6.903954937344506, rmse: 2.626683235168457, mae: 2.235442876815796\n",
      "Best model found! Loss: 6.903954937344506\n",
      "Epoch 3 train loss: 6.885153427806537, rmse: 2.624011278152466, mae: 2.209521532058716; Epoch 3 valid loss: 6.695004422324044, rmse: 2.5870778560638428, mae: 2.164466619491577\n",
      "Best model found! Loss: 6.695004422324044\n",
      "Epoch 4 train loss: 6.7201307709950315, rmse: 2.592360496520996, mae: 2.171351671218872; Epoch 4 valid loss: 6.585762078421457, rmse: 2.5661089420318604, mae: 2.1309866905212402\n",
      "Best model found! Loss: 6.585762078421457\n",
      "Epoch 5 train loss: 6.638558999251582, rmse: 2.5765442848205566, mae: 2.1530396938323975; Epoch 5 valid loss: 6.619171294711885, rmse: 2.5721235275268555, mae: 2.157093048095703\n",
      "Epoch 6 train loss: 6.632006760953243, rmse: 2.575216293334961, mae: 2.14910888671875; Epoch 6 valid loss: 6.569871271224249, rmse: 2.5626749992370605, mae: 2.1412007808685303\n",
      "Best model found! Loss: 6.569871271224249\n",
      "Epoch 7 train loss: 6.549635711906035, rmse: 2.559070110321045, mae: 2.12809419631958; Epoch 7 valid loss: 6.6066155410948255, rmse: 2.5696616172790527, mae: 2.1475913524627686\n",
      "Epoch 8 train loss: 6.5445277575598, rmse: 2.558201551437378, mae: 2.1304233074188232; Epoch 8 valid loss: 6.530820010957264, rmse: 2.5559284687042236, mae: 2.137657880783081\n",
      "Best model found! Loss: 6.530820010957264\n",
      "Epoch 9 train loss: 6.46764193604947, rmse: 2.543093681335449, mae: 2.113086223602295; Epoch 9 valid loss: 6.533023386909848, rmse: 2.555229663848877, mae: 2.126680612564087\n",
      "Epoch 10 train loss: 6.419233406535431, rmse: 2.533670663833618, mae: 2.099456787109375; Epoch 10 valid loss: 6.38354165440514, rmse: 2.525876522064209, mae: 2.0678517818450928\n",
      "Best model found! Loss: 6.38354165440514\n",
      "Epoch 11 train loss: 6.2906709238454495, rmse: 2.5081183910369873, mae: 2.0741190910339355; Epoch 11 valid loss: 6.395955308278402, rmse: 2.5285205841064453, mae: 2.0792226791381836\n",
      "Epoch 12 train loss: 6.3180805608428425, rmse: 2.5135748386383057, mae: 2.0779123306274414; Epoch 12 valid loss: 6.35437715167091, rmse: 2.5199127197265625, mae: 2.087333917617798\n",
      "Best model found! Loss: 6.35437715167091\n",
      "Epoch 13 train loss: 6.24431019733215, rmse: 2.4987103939056396, mae: 2.0612382888793945; Epoch 13 valid loss: 6.222347961153303, rmse: 2.493161201477051, mae: 2.0648696422576904\n",
      "Best model found! Loss: 6.222347961153303\n",
      "Epoch 14 train loss: 6.272099454813354, rmse: 2.5043509006500244, mae: 2.0647847652435303; Epoch 14 valid loss: 6.267945686976115, rmse: 2.502856731414795, mae: 2.080872058868408\n",
      "Epoch 15 train loss: 6.203641756821416, rmse: 2.490584135055542, mae: 2.050978899002075; Epoch 15 valid loss: 6.350034127916609, rmse: 2.519359588623047, mae: 2.09523606300354\n",
      "Epoch 16 train loss: 6.150193377666363, rmse: 2.4798829555511475, mae: 2.0413620471954346; Epoch 16 valid loss: 6.199855522882371, rmse: 2.488884925842285, mae: 2.0286149978637695\n",
      "Best model found! Loss: 6.199855522882371\n",
      "Epoch 17 train loss: 6.032225021771813, rmse: 2.455899238586426, mae: 2.0122973918914795; Epoch 17 valid loss: 6.20317131451198, rmse: 2.488638162612915, mae: 2.0694644451141357\n",
      "Epoch 18 train loss: 6.071291609013334, rmse: 2.4638876914978027, mae: 2.0211737155914307; Epoch 18 valid loss: 6.257265338443575, rmse: 2.500298023223877, mae: 2.055161952972412\n",
      "Epoch 19 train loss: 6.003443788513685, rmse: 2.450124979019165, mae: 2.0073561668395996; Epoch 19 valid loss: 6.218024526323591, rmse: 2.4909515380859375, mae: 2.080854654312134\n",
      "Epoch 20 train loss: 5.980017174143395, rmse: 2.4452857971191406, mae: 2.0028088092803955; Epoch 20 valid loss: 6.095574792226156, rmse: 2.467247724533081, mae: 2.0399301052093506\n",
      "Best model found! Loss: 6.095574792226156\n",
      "Epoch 21 train loss: 5.883987333834517, rmse: 2.4254813194274902, mae: 1.9795279502868652; Epoch 21 valid loss: 6.1614416826339, rmse: 2.4804248809814453, mae: 2.047307014465332\n",
      "Epoch 22 train loss: 5.990000847921611, rmse: 2.4473257064819336, mae: 2.003934621810913; Epoch 22 valid loss: 6.367190733410063, rmse: 2.522296667098999, mae: 2.0804619789123535\n",
      "Epoch 23 train loss: 5.878094126223596, rmse: 2.4242615699768066, mae: 1.9781955480575562; Epoch 23 valid loss: 6.190346311387562, rmse: 2.485743522644043, mae: 2.024228096008301\n",
      "Epoch 24 train loss: 5.940525256918754, rmse: 2.4370803833007812, mae: 1.9908812046051025; Epoch 24 valid loss: 6.2266397862207326, rmse: 2.4937078952789307, mae: 2.0205256938934326\n",
      "Epoch 25 train loss: 5.816046871806943, rmse: 2.4115183353424072, mae: 1.9629502296447754; Epoch 25 valid loss: 6.239976580937704, rmse: 2.497520923614502, mae: 2.037630319595337\n",
      "Epoch 26 train loss: 5.883486338003, rmse: 2.4256248474121094, mae: 1.9786514043807983; Epoch 26 valid loss: 6.427805305662609, rmse: 2.534179449081421, mae: 2.061384916305542\n",
      "Epoch 27 train loss: 5.756298568769857, rmse: 2.399118423461914, mae: 1.9482582807540894; Epoch 27 valid loss: 6.246142376036871, rmse: 2.4978549480438232, mae: 2.026268243789673\n",
      "Epoch 28 train loss: 5.783261722476155, rmse: 2.404474973678589, mae: 1.9572330713272095; Epoch 28 valid loss: 6.296923889432635, rmse: 2.5074732303619385, mae: 2.0796866416931152\n",
      "Epoch 29 train loss: 5.734825534101151, rmse: 2.3945233821868896, mae: 1.9413479566574097; Epoch 29 valid loss: 6.28663911819458, rmse: 2.507009267807007, mae: 2.0604488849639893\n",
      "Epoch 30 train loss: 5.706647226381579, rmse: 2.3887760639190674, mae: 1.9363378286361694; Epoch 30 valid loss: 6.028937734876361, rmse: 2.4541015625, mae: 1.9904303550720215\n",
      "Best model found! Loss: 6.028937734876361\n",
      "Epoch 31 train loss: 5.677132687208961, rmse: 2.382586717605591, mae: 1.9310050010681152; Epoch 31 valid loss: 6.064167583556403, rmse: 2.461310625076294, mae: 2.009361982345581\n",
      "Epoch 32 train loss: 5.657643071918008, rmse: 2.3784239292144775, mae: 1.9235936403274536; Epoch 32 valid loss: 5.981105654580253, rmse: 2.443638801574707, mae: 1.988729476928711\n",
      "Best model found! Loss: 5.981105654580253\n",
      "Epoch 33 train loss: 5.585867586403336, rmse: 2.3632776737213135, mae: 1.9073058366775513; Epoch 33 valid loss: 6.279680772054763, rmse: 2.5042836666107178, mae: 2.035783529281616\n",
      "Epoch 34 train loss: 5.529532498501717, rmse: 2.351341962814331, mae: 1.892715573310852; Epoch 34 valid loss: 6.298280933925084, rmse: 2.5095489025115967, mae: 2.0364506244659424\n",
      "Epoch 35 train loss: 5.622368557310197, rmse: 2.3710544109344482, mae: 1.921722412109375; Epoch 35 valid loss: 6.16914058185759, rmse: 2.4816930294036865, mae: 2.0337657928466797\n",
      "Epoch 36 train loss: 5.578807118552335, rmse: 2.361668586730957, mae: 1.915217399597168; Epoch 36 valid loss: 6.321894686562675, rmse: 2.5118587017059326, mae: 2.033799886703491\n",
      "Epoch 37 train loss: 5.526438348067337, rmse: 2.350666046142578, mae: 1.898773431777954; Epoch 37 valid loss: 6.368700924373808, rmse: 2.521979808807373, mae: 2.0516886711120605\n",
      "Epoch 38 train loss: 5.505622721732931, rmse: 2.3459742069244385, mae: 1.8890742063522339; Epoch 38 valid loss: 6.247616191137404, rmse: 2.4968621730804443, mae: 2.036813259124756\n",
      "Epoch 39 train loss: 5.4074375539499275, rmse: 2.3251616954803467, mae: 1.8717769384384155; Epoch 39 valid loss: 6.235821122214908, rmse: 2.4960076808929443, mae: 2.0413382053375244\n",
      "Epoch 40 train loss: 5.414754755492368, rmse: 2.3269448280334473, mae: 1.8725265264511108; Epoch 40 valid loss: 6.399999981834775, rmse: 2.527679681777954, mae: 2.048220634460449\n",
      "Epoch 41 train loss: 5.359819376491716, rmse: 2.3148927688598633, mae: 1.8578299283981323; Epoch 41 valid loss: 6.308568927219936, rmse: 2.5115604400634766, mae: 2.0396499633789062\n",
      "Epoch 42 train loss: 5.399760671003183, rmse: 2.323470115661621, mae: 1.8622655868530273; Epoch 42 valid loss: 6.270857529413132, rmse: 2.5017433166503906, mae: 2.037602424621582\n",
      "Epoch 43 train loss: 5.380561229569308, rmse: 2.3193347454071045, mae: 1.8608249425888062; Epoch 43 valid loss: 6.510002815155756, rmse: 2.5482654571533203, mae: 2.086080551147461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 train loss: 5.341262365678754, rmse: 2.3110499382019043, mae: 1.8525022268295288; Epoch 44 valid loss: 6.316952421551659, rmse: 2.5107603073120117, mae: 2.0293543338775635\n",
      "Epoch 45 train loss: 5.168272582889756, rmse: 2.273165702819824, mae: 1.816643476486206; Epoch 45 valid loss: 6.300575526555379, rmse: 2.505676031112671, mae: 2.0319695472717285\n",
      "Epoch 46 train loss: 5.069834111506981, rmse: 2.2512969970703125, mae: 1.789899468421936; Epoch 46 valid loss: 6.287187024525234, rmse: 2.5024640560150146, mae: 2.0405406951904297\n",
      "Epoch 47 train loss: 5.185432671347716, rmse: 2.276857852935791, mae: 1.816666841506958; Epoch 47 valid loss: 6.228671475819179, rmse: 2.4917073249816895, mae: 1.9906742572784424\n",
      "Epoch 48 train loss: 5.054673278354814, rmse: 2.248030662536621, mae: 1.7860331535339355; Epoch 48 valid loss: 6.205904404322307, rmse: 2.4880356788635254, mae: 1.9929934740066528\n",
      "Epoch 49 train loss: 5.053473495191946, rmse: 2.2475016117095947, mae: 1.7878284454345703; Epoch 49 valid loss: 6.315276976994106, rmse: 2.508441209793091, mae: 2.018077850341797\n",
      "Epoch 50 train loss: 5.114686959716753, rmse: 2.261301279067993, mae: 1.7992042303085327; Epoch 50 valid loss: 6.477536875861031, rmse: 2.5406365394592285, mae: 2.043107509613037\n",
      "Epoch 51 train loss: 5.017912077834574, rmse: 2.2399661540985107, mae: 1.786794662475586; Epoch 51 valid loss: 6.255395135425386, rmse: 2.4975790977478027, mae: 2.017090082168579\n",
      "Epoch 52 train loss: 4.9226183856709556, rmse: 2.218259572982788, mae: 1.760585904121399; Epoch 52 valid loss: 6.260699090503511, rmse: 2.497666597366333, mae: 1.9998151063919067\n",
      "Epoch 53 train loss: 4.912452229217588, rmse: 2.216101884841919, mae: 1.7566689252853394; Epoch 53 valid loss: 6.464050547281901, rmse: 2.5375759601593018, mae: 2.0305848121643066\n",
      "Epoch 54 train loss: 4.8479075438741095, rmse: 2.2010152339935303, mae: 1.7392516136169434; Epoch 54 valid loss: 6.442690006891886, rmse: 2.534010887145996, mae: 2.0314242839813232\n",
      "Epoch 55 train loss: 4.805189277955135, rmse: 2.191962957382202, mae: 1.7313499450683594; Epoch 55 valid loss: 6.4284451348440985, rmse: 2.531146764755249, mae: 2.011770248413086\n",
      "Epoch 56 train loss: 4.848407711927388, rmse: 2.201770305633545, mae: 1.7418267726898193; Epoch 56 valid loss: 6.277436623119173, rmse: 2.5033786296844482, mae: 1.9942798614501953\n",
      "Epoch 57 train loss: 4.890596846316723, rmse: 2.211315631866455, mae: 1.7511790990829468; Epoch 57 valid loss: 6.198666182018462, rmse: 2.482335329055786, mae: 2.0047245025634766\n",
      "Epoch 58 train loss: 4.962381399231215, rmse: 2.227449417114258, mae: 1.7689237594604492; Epoch 58 valid loss: 6.362047379357474, rmse: 2.5179519653320312, mae: 2.0150198936462402\n",
      "Epoch 59 train loss: 4.7795225430272765, rmse: 2.1862258911132812, mae: 1.7285133600234985; Epoch 59 valid loss: 6.343745676676432, rmse: 2.514298677444458, mae: 2.0207479000091553\n",
      "Epoch 60 train loss: 4.865749778553872, rmse: 2.205293893814087, mae: 1.7419376373291016; Epoch 60 valid loss: 6.283992785499209, rmse: 2.5022671222686768, mae: 2.0254099369049072\n",
      "Epoch 61 train loss: 4.68775731636431, rmse: 2.1650493144989014, mae: 1.7064170837402344; Epoch 61 valid loss: 6.537092465446109, rmse: 2.5545990467071533, mae: 2.0407307147979736\n",
      "Epoch 62 train loss: 4.664342645634997, rmse: 2.159517765045166, mae: 1.6990628242492676; Epoch 62 valid loss: 6.4002633367265975, rmse: 2.524797201156616, mae: 2.014573812484741\n",
      "Epoch 63 train loss: 4.568273112667829, rmse: 2.1372179985046387, mae: 1.6799538135528564; Epoch 63 valid loss: 6.436333976473127, rmse: 2.5322275161743164, mae: 2.037595748901367\n",
      "Epoch 64 train loss: 4.64729686747206, rmse: 2.155205488204956, mae: 1.6971410512924194; Epoch 64 valid loss: 6.454106616973877, rmse: 2.53684139251709, mae: 2.0317139625549316\n",
      "Epoch 65 train loss: 4.710476375755996, rmse: 2.170351266860962, mae: 1.715326189994812; Epoch 65 valid loss: 6.222709496815999, rmse: 2.490108013153076, mae: 2.0047473907470703\n",
      "Epoch 66 train loss: 4.720718387363036, rmse: 2.172700881958008, mae: 1.7117725610733032; Epoch 66 valid loss: 6.285906273978097, rmse: 2.5021724700927734, mae: 2.0009093284606934\n",
      "Epoch 67 train loss: 4.515835597160015, rmse: 2.1249303817749023, mae: 1.6665120124816895; Epoch 67 valid loss: 6.241289900598072, rmse: 2.492266893386841, mae: 1.9885624647140503\n",
      "Epoch 68 train loss: 4.526142687008966, rmse: 2.1274452209472656, mae: 1.6681135892868042; Epoch 68 valid loss: 6.306377841177441, rmse: 2.5039222240448, mae: 1.9972585439682007\n",
      "Epoch 69 train loss: 4.528512637435353, rmse: 2.1276743412017822, mae: 1.6685456037521362; Epoch 69 valid loss: 6.60303426583608, rmse: 2.5628435611724854, mae: 2.026825428009033\n",
      "Epoch 70 train loss: 4.524971628096855, rmse: 2.1270854473114014, mae: 1.6701444387435913; Epoch 70 valid loss: 6.5993035112108505, rmse: 2.5600593090057373, mae: 2.038325786590576\n",
      "Epoch 71 train loss: 4.329628579736448, rmse: 2.080430269241333, mae: 1.622620701789856; Epoch 71 valid loss: 6.753662518092564, rmse: 2.593222141265869, mae: 2.0777270793914795\n",
      "Epoch 72 train loss: 4.3020209005999615, rmse: 2.0740244388580322, mae: 1.6154154539108276; Epoch 72 valid loss: 6.5198047830944965, rmse: 2.549471378326416, mae: 2.009523868560791\n",
      "Epoch 73 train loss: 4.353827366284753, rmse: 2.086564540863037, mae: 1.629569172859192; Epoch 73 valid loss: 6.614420493443807, rmse: 2.565736770629883, mae: 2.031521797180176\n",
      "Epoch 74 train loss: 4.346511399838173, rmse: 2.0844695568084717, mae: 1.6256119012832642; Epoch 74 valid loss: 6.387999456269401, rmse: 2.521765947341919, mae: 2.007833480834961\n",
      "Epoch 75 train loss: 4.222025743074989, rmse: 2.054744243621826, mae: 1.6011736392974854; Epoch 75 valid loss: 6.5832885628654845, rmse: 2.557394504547119, mae: 2.0301132202148438\n",
      "Epoch 76 train loss: 4.382064768839159, rmse: 2.0930190086364746, mae: 1.6326957941055298; Epoch 76 valid loss: 6.570111045383272, rmse: 2.556640386581421, mae: 2.0424180030822754\n",
      "Epoch 77 train loss: 4.395299892706843, rmse: 2.096198558807373, mae: 1.6375054121017456; Epoch 77 valid loss: 6.525471274058024, rmse: 2.55192232131958, mae: 2.0279109477996826\n",
      "Epoch 78 train loss: 4.325596089981049, rmse: 2.07955002784729, mae: 1.6151658296585083; Epoch 78 valid loss: 6.5019485337393625, rmse: 2.5382134914398193, mae: 2.0191471576690674\n",
      "Epoch 79 train loss: 4.559515647777506, rmse: 2.135331869125366, mae: 1.6758317947387695; Epoch 79 valid loss: 6.300442054158165, rmse: 2.503875494003296, mae: 1.9785698652267456\n",
      "Epoch 80 train loss: 4.272671760857913, rmse: 2.0666580200195312, mae: 1.6098835468292236; Epoch 80 valid loss: 6.518525979632423, rmse: 2.5452044010162354, mae: 2.025296688079834\n",
      "Epoch 81 train loss: 4.192038980051443, rmse: 2.0472800731658936, mae: 1.59042489528656; Epoch 81 valid loss: 6.387594416027977, rmse: 2.5219688415527344, mae: 1.9971853494644165\n",
      "Epoch 82 train loss: 4.1186643820436135, rmse: 2.029311418533325, mae: 1.5753567218780518; Epoch 82 valid loss: 6.536506646020072, rmse: 2.555558681488037, mae: 2.044218063354492\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_fst(n_epochs, train_dataloader2, test_dataloader2, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 5.981105654580253, rmse 2.443638801574707, mae 1.988729476928711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.981105654580253, 2.443638801574707, 1.988729476928711)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def rmse(predictions, targets):\n",
    "    return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "def mae(predictions, targets):\n",
    "    return torch.mean(torch.abs(predictions - targets))\n",
    "\n",
    "def test2(test_dataloader):\n",
    "    best_model = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, rmse {rmse_error}, mae {mae_error}')\n",
    "        \n",
    "        return valid_loss, rmse_error.item(), mae_error.item()\n",
    "        \n",
    "test2(test_dataloader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = \"/home/wmnlab/Documents/YU/model\"\n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "torch.save(classifier.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# m_path = os.path.join('/home/wmnlab/Documents/sheng-ru/model', 'lte_HO_cls_RNN.pt')\n",
    "# classifier = RNN(input_dim, out_dim, hidden_dim, num_layers, dropout, rnn)\n",
    "# classifier.load_state_dict(torch.load(m_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
