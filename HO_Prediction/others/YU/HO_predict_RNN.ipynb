{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ex.\r\n",
      "# Now in use: ccc\r\n",
      "# Start time: 03/31 12:00\r\n",
      "# Estimated end time(option): 03/31 18:00\r\n",
      "\r\n",
      "Now in use: \r\n",
      "Start time:  \r\n",
      "Estimated end time(option):  \r\n",
      "\r\n",
      "Next one who want to use: \r\n",
      "Estimated start time:\r\n",
      "Estimated end time(option): \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ~/user_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 25 19:33:20 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4090         On | 00000000:01:00.0 Off |                  Off |\r\n",
      "| 30%   42C    P8               44W / 480W|    556MiB / 24564MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      1158      G   /usr/lib/xorg/Xorg                            4MiB |\r\n",
      "|    0   N/A  N/A   2951009      C   .../anaconda3/envs/sheng-ru/bin/python      548MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_array_create(dirname, dir_list, time_seq):\n",
    "    \n",
    "    columns = ['RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "               'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2']\n",
    "    \n",
    "    def reamin_HO_time(y_train):\n",
    "        def f(L):    \n",
    "            for i, e in enumerate(L):\n",
    "                if e: return i+1\n",
    "            return 0\n",
    "\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            a1_out = []\n",
    "            for a1 in a2:\n",
    "                a1_out.append(a1.any())\n",
    "      \n",
    "            out.append(f(a1_out))\n",
    "        return out\n",
    "    \n",
    "    def HO(y_train):\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            if sum(a2.reshape(-1)) == 0: ho = 0\n",
    "            elif sum(a2.reshape(-1)) > 0: ho = 1\n",
    "            out.append(ho)\n",
    "        return out\n",
    "\n",
    "    split_time = []\n",
    "    for i, f in enumerate(tqdm(dir_list)):\n",
    "    \n",
    "        f = os.path.join(dirname, f)\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # preprocess data with ffill method\n",
    "        del df['Timestamp'], df['lat'], df['long'], df['gpsspeed']\n",
    "        # df[columns] = df[columns].replace(0, np.nan)\n",
    "        # df[columns] = df[columns].fillna(method='ffill')\n",
    "        # df.dropna(inplace=True)\n",
    "        \n",
    "        df.replace(np.nan,0,inplace=True); df.replace('-',0,inplace=True)\n",
    "        \n",
    "        X = df[features]\n",
    "        Y = df[target]\n",
    "\n",
    "        Xt_list = []\n",
    "        Yt_list = []\n",
    "\n",
    "        for j in range(time_seq):\n",
    "            X_t = X.shift(periods=-j)\n",
    "            Xt_list.append(X_t)\n",
    "    \n",
    "        for j in range(time_seq,time_seq+predict_t):\n",
    "            Y_t = Y.shift(periods=-(j))\n",
    "            Yt_list.append(Y_t)\n",
    "\n",
    "        # YY = Y.shift(periods=-(0))\n",
    "\n",
    "        X_ts = np.array(Xt_list); X_ts = np.transpose(X_ts, (1,0,2)); X_ts = X_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        Y_ts = np.array(Yt_list); Y_ts = np.transpose(Y_ts, (1,0,2)); Y_ts = Y_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        split_time.append(len(X_ts))\n",
    "\n",
    "        if i == 0:\n",
    "            X_final = X_ts\n",
    "            Y_final = Y_ts\n",
    "        else:\n",
    "            X_final = np.concatenate((X_final,X_ts), axis=0)\n",
    "            Y_final = np.concatenate((Y_final,Y_ts), axis=0)\n",
    "\n",
    "    split_time = [(sum(split_time[:i]), sum(split_time[:i])+x) for i, x in enumerate(split_time)]\n",
    "    \n",
    "    return X_final, np.array(HO(Y_final)), np.array(reamin_HO_time(Y_final)), split_time # forecast HO\n",
    "\n",
    "class RNN_Dataset_simple(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset take all csv file specified in dir_list in directory dirname.\n",
    "    Transfer csvs to (features, label) pair\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.inputs = torch.FloatTensor(X)\n",
    "        self.labels = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_file(file, dates):\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in file: return True \n",
    "    return False\n",
    "\n",
    "def train_valid_split(L, valid_size=0.2):\n",
    "    \n",
    "    length = len(L)\n",
    "    v_num = int(length*valid_size)\n",
    "    v_files = random.sample(L, v_num)\n",
    "    t_files = list(set(L) - set(v_files))\n",
    "    \n",
    "    return t_files, v_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n",
    "    \n",
    "    print(f\"Random seed set as {seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time sequence length and prediction time length\n",
    "seed = 55688\n",
    "time_seq = 20\n",
    "predict_t = 10\n",
    "valid_ratio = 0.2\n",
    "task = 'classification'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "Loading training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c665e171a794025aa991ff68776c6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57fcf0ffb9844e895d2ca05cb9e5949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup seed\n",
    "set_seed(seed)\n",
    "\n",
    "# Get GPU\n",
    "device_count = torch.cuda.device_count()\n",
    "num_of_gpus = device_count\n",
    "\n",
    "for i in range(device_count):\n",
    "    print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "    gpu_id = i\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Save best model to \n",
    "save_path = \"/home/wmnlab/Documents/sheng-ru/YU/model\"\n",
    "\n",
    "# Define DataSet\n",
    "dirname = \"/home/wmnlab/Documents/sheng-ru/data/single\"\n",
    "# dirname = \"/home/wmnlab/Documents/sheng-ru/data/single0.5\"\n",
    "dir_list = os.listdir(dirname)\n",
    "dir_list = [f for f in dir_list if ( f.endswith('.csv') and (not 'sm' in f) ) ]\n",
    "\n",
    "train_dates = ['03-26', '04-01']\n",
    "test_dates = ['04-10']\n",
    "    \n",
    "# train_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, train_dates) )]\n",
    "# test_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, test_dates) )]\n",
    "\n",
    "train_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio)\n",
    "train_dir_list += [f for f in os.listdir(dirname) if 'sm' in f]\n",
    "\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "#         'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2' ]\n",
    "features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "        'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1','nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2']\n",
    "\n",
    "num_of_features = len(features)\n",
    "\n",
    "# target = ['LTE_HO', 'MN_HO'] # For eNB HO.\n",
    "# target = ['eNB_to_ENDC'] # Setup gNB\n",
    "target = ['gNB_Rel', 'gNB_HO'] # For gNB HO.\n",
    "# target = ['RLF'] # For RLF\n",
    "# target = ['SCG_RLF'] # For scg failure\n",
    "# target = ['dl-loss'] # For DL loss\n",
    "# target = ['ul-loss'] # For UL loss\n",
    "\n",
    "# Data\n",
    "print('Loading training data...')\n",
    "X_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq)\n",
    "\n",
    "train_dataset = RNN_Dataset_simple(X_train, y_train1)\n",
    "train_dataloader1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_train2 > 0\n",
    "X_train_fore = X_train[cond]\n",
    "y_train2_fore = y_train2[cond]\n",
    "train_dataset = RNN_Dataset_simple(X_train_fore, y_train2_fore)\n",
    "train_dataloader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Loading testing data...')\n",
    "X_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq)\n",
    "\n",
    "test_dataset = RNN_Dataset_simple(X_test, y_test1)\n",
    "test_dataloader1 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_test2 > 0\n",
    "X_test_fore = X_test[cond]\n",
    "y_test2_fore = y_test2[cond]\n",
    "test_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\n",
    "test_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = next(iter(train_dataloader1))\n",
    "input_dim, out_dim = a.shape[2], 1\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Cls(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "        \n",
    "        out = torch.sigmoid(out) # Binary Classifier\n",
    "\n",
    "        return out\n",
    "\n",
    "class RNN_Fst(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "# Define model and optimizer\n",
    "\n",
    "classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[600, 1000], gamma=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        classifier.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = classifier(features)\n",
    "            \n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, auc: {roc_auc}, aucpr: {aucpr}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        classifier.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = classifier(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, auc: {roc_auc}, aucpr: {aucpr}')\n",
    "        \n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(classifier.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.4774140861013429, auc: 0.7496810371895504, aucpr: 0.4570211079702917; Epoch 1 valid loss: 0.5086425727002417, auc: 0.7763608982041057, aucpr: 0.5866705567324189\n",
      "Best model found! Loss: 0.5086425727002417\n",
      "Epoch 2 train loss: 0.4584015765887609, auc: 0.7871294142289098, aucpr: 0.5275495477999289; Epoch 2 valid loss: 0.4936919277470301, auc: 0.7876283184875957, aucpr: 0.6047966332800178\n",
      "Best model found! Loss: 0.4936919277470301\n",
      "Epoch 3 train loss: 0.44714903059819466, auc: 0.8055753762762332, aucpr: 0.5652296955480818; Epoch 3 valid loss: 0.5253952310507151, auc: 0.7863431284831807, aucpr: 0.6053279356678947\n",
      "Epoch 4 train loss: 0.4357925690394512, auc: 0.8211278469362573, aucpr: 0.5964203010461928; Epoch 4 valid loss: 0.47461346474546107, auc: 0.8102738888668259, aucpr: 0.6496881555461942\n",
      "Best model found! Loss: 0.47461346474546107\n",
      "Epoch 5 train loss: 0.43167550927850223, auc: 0.8269419755499636, aucpr: 0.6060895302642142; Epoch 5 valid loss: 0.48012119740665454, auc: 0.8076773448517256, aucpr: 0.6413301872867763\n",
      "Epoch 6 train loss: 0.4259393239894147, auc: 0.833874023387446, aucpr: 0.6185005165084698; Epoch 6 valid loss: 0.46528469004089645, auc: 0.8218935109370996, aucpr: 0.6621624497337175\n",
      "Best model found! Loss: 0.46528469004089645\n",
      "Epoch 7 train loss: 0.42273457346442106, auc: 0.837541594693809, aucpr: 0.622093380205947; Epoch 7 valid loss: 0.47945465815831717, auc: 0.8127629735704602, aucpr: 0.6566610531416233\n",
      "Epoch 8 train loss: 0.41587862054693175, auc: 0.8454254009647874, aucpr: 0.6430339777627145; Epoch 8 valid loss: 0.46026469364176803, auc: 0.8289388315123651, aucpr: 0.6712003368858352\n",
      "Best model found! Loss: 0.46026469364176803\n",
      "Epoch 9 train loss: 0.4102167993374779, auc: 0.8512266870044287, aucpr: 0.6531407604758894; Epoch 9 valid loss: 0.451366263276351, auc: 0.8354931097556701, aucpr: 0.6817885545001368\n",
      "Best model found! Loss: 0.451366263276351\n",
      "Epoch 10 train loss: 0.40345725830037826, auc: 0.8580729040647443, aucpr: 0.6671615753828541; Epoch 10 valid loss: 0.4624390071287603, auc: 0.827492218933552, aucpr: 0.670676059488723\n",
      "Epoch 11 train loss: 0.40779788792324995, auc: 0.8539577971793054, aucpr: 0.657420626086661; Epoch 11 valid loss: 0.4533264354620525, auc: 0.834201768095019, aucpr: 0.6800800954274185\n",
      "Epoch 12 train loss: 0.4075398326398654, auc: 0.8538316791386238, aucpr: 0.6596319510253836; Epoch 12 valid loss: 0.4548876932149906, auc: 0.8318321734735911, aucpr: 0.6665798795986427\n",
      "Epoch 13 train loss: 0.41044278293979797, auc: 0.8497778365733419, aucpr: 0.6515322915571321; Epoch 13 valid loss: 0.5030633023398732, auc: 0.756662555179967, aucpr: 0.5476883750690922\n",
      "Epoch 14 train loss: 0.40960843465212704, auc: 0.8507400116193012, aucpr: 0.6510645438984413; Epoch 14 valid loss: 0.45071865145132695, auc: 0.8371532782487558, aucpr: 0.6862796351307139\n",
      "Best model found! Loss: 0.45071865145132695\n",
      "Epoch 15 train loss: 0.3989032875823864, auc: 0.8615661816963799, aucpr: 0.673536556415614; Epoch 15 valid loss: 0.44778201625714636, auc: 0.8357515182521198, aucpr: 0.6768318801474864\n",
      "Best model found! Loss: 0.44778201625714636\n",
      "Epoch 16 train loss: 0.39669236631987553, auc: 0.8641544270244987, aucpr: 0.6763520206059626; Epoch 16 valid loss: 0.44485487391198186, auc: 0.840319668090892, aucpr: 0.6870000945136004\n",
      "Best model found! Loss: 0.44485487391198186\n",
      "Epoch 17 train loss: 0.38825391430916983, auc: 0.8720750254483052, aucpr: 0.6917198876604008; Epoch 17 valid loss: 0.43744630100467813, auc: 0.8469969015898527, aucpr: 0.6956483536600256\n",
      "Best model found! Loss: 0.43744630100467813\n",
      "Epoch 18 train loss: 0.387111488634666, auc: 0.8727347080133712, aucpr: 0.6974113125408867; Epoch 18 valid loss: 0.44378101605881215, auc: 0.8442926257215544, aucpr: 0.6872145206499164\n",
      "Epoch 19 train loss: 0.3900534226756214, auc: 0.8691481906730898, aucpr: 0.6911349551097807; Epoch 19 valid loss: 0.44059008188576304, auc: 0.8436954478522545, aucpr: 0.6915716193822515\n",
      "Epoch 20 train loss: 0.3901792910122365, auc: 0.8693239578238361, aucpr: 0.6937769332405852; Epoch 20 valid loss: 0.44054191724988706, auc: 0.8447028788972495, aucpr: 0.6917543302242473\n",
      "Epoch 21 train loss: 0.38917413000911333, auc: 0.8703225894152872, aucpr: 0.6934283119736349; Epoch 21 valid loss: 0.46540780283606437, auc: 0.8193673994924494, aucpr: 0.6444149908491641\n",
      "Epoch 22 train loss: 0.38345697635858, auc: 0.8747314965516122, aucpr: 0.7058304097631276; Epoch 22 valid loss: 0.44015718335832704, auc: 0.8464756071256425, aucpr: 0.7067474158569944\n",
      "Epoch 23 train loss: 0.38591179131673675, auc: 0.8740136660202251, aucpr: 0.7024672324231547; Epoch 23 valid loss: 0.4449285617295787, auc: 0.8414447203605026, aucpr: 0.6908478431769515\n",
      "Epoch 24 train loss: 0.382451142329978, auc: 0.8761755989379568, aucpr: 0.705285846121862; Epoch 24 valid loss: 0.4528564724085743, auc: 0.8322863837372013, aucpr: 0.6781030453905678\n",
      "Epoch 25 train loss: 0.3924489796653197, auc: 0.8672721344768496, aucpr: 0.6875184883036056; Epoch 25 valid loss: 0.4558282878163734, auc: 0.8295369048759271, aucpr: 0.6623902717122121\n",
      "Epoch 26 train loss: 0.3833284223250119, auc: 0.8750707636059017, aucpr: 0.7023695454875822; Epoch 26 valid loss: 0.4573736499904869, auc: 0.8220727265931427, aucpr: 0.6536104427768561\n",
      "Epoch 27 train loss: 0.3769348760092951, auc: 0.8794916503259987, aucpr: 0.7149834049681334; Epoch 27 valid loss: 0.4538085686065474, auc: 0.8311162452782146, aucpr: 0.664710826837457\n",
      "Epoch 28 train loss: 0.38843963359876826, auc: 0.8715039703882077, aucpr: 0.6900437047431538; Epoch 28 valid loss: 0.4463405034787592, auc: 0.842416604176741, aucpr: 0.6911048098869104\n",
      "Epoch 29 train loss: 0.38131828101847903, auc: 0.8768921669465077, aucpr: 0.7063878373851189; Epoch 29 valid loss: 0.4388712803192482, auc: 0.8535852086151221, aucpr: 0.705982849914452\n",
      "Epoch 30 train loss: 0.38296421309715206, auc: 0.8752297775580512, aucpr: 0.7021818512287517; Epoch 30 valid loss: 0.4483236812813721, auc: 0.8398668594704747, aucpr: 0.6890487721145125\n",
      "Epoch 31 train loss: 0.3912345500797816, auc: 0.8676449111245931, aucpr: 0.6884271749115621; Epoch 31 valid loss: 0.43069112364439033, auc: 0.8561735763782631, aucpr: 0.7159990297866808\n",
      "Best model found! Loss: 0.43069112364439033\n",
      "Epoch 32 train loss: 0.38484507182987565, auc: 0.8736555314411206, aucpr: 0.6959612983140613; Epoch 32 valid loss: 0.4499452973693774, auc: 0.837354491916023, aucpr: 0.6761635978936393\n",
      "Epoch 33 train loss: 0.3799242136197135, auc: 0.8781015247862255, aucpr: 0.7141325027143851; Epoch 33 valid loss: 0.4267850054742592, auc: 0.8575774777742218, aucpr: 0.7170222933661575\n",
      "Best model found! Loss: 0.4267850054742592\n",
      "Epoch 34 train loss: 0.37238295731739424, auc: 0.882841594858736, aucpr: 0.7202373473308156; Epoch 34 valid loss: 0.42767101193420376, auc: 0.8603133295488966, aucpr: 0.7259658703162515\n",
      "Epoch 35 train loss: 0.3751962408109833, auc: 0.880859708884352, aucpr: 0.7178179562999061; Epoch 35 valid loss: 0.43697941552617514, auc: 0.8600709620801008, aucpr: 0.722788721007426\n",
      "Epoch 36 train loss: 0.37044293964280733, auc: 0.8847171700177943, aucpr: 0.7276992319375504; Epoch 36 valid loss: 0.42889282211323837, auc: 0.858317545380145, aucpr: 0.7151586600521457\n",
      "Epoch 37 train loss: 0.3687395600949277, auc: 0.8863809385925896, aucpr: 0.7303108761354147; Epoch 37 valid loss: 0.42667869231388095, auc: 0.8581143071171548, aucpr: 0.7177370218051893\n",
      "Best model found! Loss: 0.42667869231388095\n",
      "Epoch 38 train loss: 0.37031643884782, auc: 0.885799922387128, aucpr: 0.7291361529727629; Epoch 38 valid loss: 0.43882171613977083, auc: 0.8452018638739704, aucpr: 0.6959929109858993\n",
      "Epoch 39 train loss: 0.369268120522824, auc: 0.8853376168889989, aucpr: 0.7257434589741876; Epoch 39 valid loss: 0.4692789333215681, auc: 0.8106125025015437, aucpr: 0.6358039404456863\n",
      "Epoch 40 train loss: 0.3633290365011951, auc: 0.8909790403693691, aucpr: 0.7378546018009495; Epoch 40 valid loss: 0.4149886352890764, auc: 0.8689856074604795, aucpr: 0.7420829169880506\n",
      "Best model found! Loss: 0.4149886352890764\n",
      "Epoch 41 train loss: 0.36190382198733617, auc: 0.8916031869119223, aucpr: 0.7414327481099527; Epoch 41 valid loss: 0.42519623870576967, auc: 0.858165856439032, aucpr: 0.7218690308427945\n",
      "Epoch 42 train loss: 0.3559357775633823, auc: 0.8959994816049272, aucpr: 0.7544482090094156; Epoch 42 valid loss: 0.4226153568738425, auc: 0.8640915366447932, aucpr: 0.7307835552898299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 train loss: 0.3549608534748962, auc: 0.8967116851199775, aucpr: 0.7532157427678985; Epoch 43 valid loss: 0.43678385122080665, auc: 0.8545045308116215, aucpr: 0.717051214329052\n",
      "Epoch 44 train loss: 0.36715657428433196, auc: 0.8881718943571519, aucpr: 0.7385229955524405; Epoch 44 valid loss: 0.425150824238399, auc: 0.8624568313363812, aucpr: 0.7271548988261136\n",
      "Epoch 45 train loss: 0.3521573127634494, auc: 0.8984885309932481, aucpr: 0.7590751772090889; Epoch 45 valid loss: 0.434282721223679, auc: 0.8547258346979888, aucpr: 0.716253050699376\n",
      "Epoch 46 train loss: 0.354047572691184, auc: 0.8966354734883182, aucpr: 0.7538902673851471; Epoch 46 valid loss: 0.42759257529739075, auc: 0.8571832656261802, aucpr: 0.7196404767654998\n",
      "Epoch 47 train loss: 0.34826264101529825, auc: 0.900749605514352, aucpr: 0.7633059608822199; Epoch 47 valid loss: 0.4470835673799128, auc: 0.8419210454389363, aucpr: 0.6861550950576443\n",
      "Epoch 48 train loss: 0.35017986890448144, auc: 0.8992846283516961, aucpr: 0.7629917361882402; Epoch 48 valid loss: 0.4349519260197575, auc: 0.8551548932531905, aucpr: 0.7151343848711935\n",
      "Epoch 49 train loss: 0.3635609529965881, auc: 0.8904042328996269, aucpr: 0.740447042606849; Epoch 49 valid loss: 0.4408666640866153, auc: 0.8463154304563049, aucpr: 0.7037245313636289\n",
      "Epoch 50 train loss: 0.35886808631129175, auc: 0.893688000460057, aucpr: 0.7495498765879969; Epoch 50 valid loss: 0.4280533380732841, auc: 0.8634111556781734, aucpr: 0.7194833008692527\n",
      "Epoch 51 train loss: 0.3467986606654439, auc: 0.9017315845894072, aucpr: 0.7684277102475154; Epoch 51 valid loss: 0.42811319415979404, auc: 0.8608088882867014, aucpr: 0.727098820081126\n",
      "Epoch 52 train loss: 0.34907802717106207, auc: 0.9002428400348279, aucpr: 0.7656066358960978; Epoch 52 valid loss: 0.423085674623264, auc: 0.8615491894998236, aucpr: 0.7180633683172217\n",
      "Epoch 53 train loss: 0.3447913389222191, auc: 0.9031176892336668, aucpr: 0.7727025579062043; Epoch 53 valid loss: 0.41319397748132125, auc: 0.8715774014625368, aucpr: 0.7454855813183586\n",
      "Best model found! Loss: 0.41319397748132125\n",
      "Epoch 54 train loss: 0.3446967372483546, auc: 0.9026592384255134, aucpr: 0.7708044755624648; Epoch 54 valid loss: 0.4296663623431292, auc: 0.858544884119149, aucpr: 0.7258891317135389\n",
      "Epoch 55 train loss: 0.3480387126135913, auc: 0.9004663529226035, aucpr: 0.7694720810966615; Epoch 55 valid loss: 0.41977016220828045, auc: 0.8687774634463136, aucpr: 0.7338984953786782\n",
      "Epoch 56 train loss: 0.33565549293087293, auc: 0.9089572265189834, aucpr: 0.7873673442161988; Epoch 56 valid loss: 0.42027529665237284, auc: 0.8674636565600407, aucpr: 0.7360642507427856\n",
      "Epoch 57 train loss: 0.3366366529928748, auc: 0.9086377666900501, aucpr: 0.7843232879633117; Epoch 57 valid loss: 0.42610689909702826, auc: 0.865484302764274, aucpr: 0.7301033658113889\n",
      "Epoch 58 train loss: 0.3424001503711081, auc: 0.9044753375720774, aucpr: 0.7772920525909346; Epoch 58 valid loss: 0.4317767270751094, auc: 0.8586723168461159, aucpr: 0.7249571849666948\n",
      "Epoch 59 train loss: 0.34017133608743716, auc: 0.9057795540263054, aucpr: 0.7774257252837853; Epoch 59 valid loss: 0.4309665546840897, auc: 0.8612800740067608, aucpr: 0.7205729467342382\n",
      "Epoch 60 train loss: 0.3418760353934499, auc: 0.9046963733580857, aucpr: 0.7751187265554876; Epoch 60 valid loss: 0.4320566512705814, auc: 0.8671705573945633, aucpr: 0.7300333760332172\n",
      "Epoch 61 train loss: 0.33532504347932696, auc: 0.9087003750376823, aucpr: 0.787141239053462; Epoch 61 valid loss: 0.44479213704438314, auc: 0.8462739262439778, aucpr: 0.7026913602669682\n",
      "Epoch 62 train loss: 0.33416342286146977, auc: 0.9093178669661981, aucpr: 0.78561443651905; Epoch 62 valid loss: 0.4194311488048551, auc: 0.8670566738851292, aucpr: 0.7428763763623464\n",
      "Epoch 63 train loss: 0.3360436942368395, auc: 0.9082086112060863, aucpr: 0.7835260959462713; Epoch 63 valid loss: 0.44080622170530853, auc: 0.8491988051770469, aucpr: 0.7020084731575531\n",
      "Epoch 64 train loss: 0.331597159654377, auc: 0.9116404055737309, aucpr: 0.7933397113915407; Epoch 64 valid loss: 0.42985040380534156, auc: 0.8606603919773058, aucpr: 0.7213415841356026\n",
      "Epoch 65 train loss: 0.3267999800480062, auc: 0.9141403382285254, aucpr: 0.7951578559416401; Epoch 65 valid loss: 0.44657712285721163, auc: 0.8437696960069523, aucpr: 0.6943199166476488\n",
      "Epoch 66 train loss: 0.32862178825211585, auc: 0.9126110938591183, aucpr: 0.7945900964216801; Epoch 66 valid loss: 0.4579123677553788, auc: 0.8448261845637034, aucpr: 0.7003373723886849\n",
      "Epoch 67 train loss: 0.3305308324520165, auc: 0.9119425081108622, aucpr: 0.790503404163713; Epoch 67 valid loss: 0.43849680118342155, auc: 0.8556937471918469, aucpr: 0.7155214212946319\n",
      "Epoch 68 train loss: 0.3321136183864557, auc: 0.9114678954122757, aucpr: 0.7891809838005241; Epoch 68 valid loss: 0.41821776417369844, auc: 0.8690655789915488, aucpr: 0.7432225994224654\n",
      "Epoch 69 train loss: 0.3313351696390913, auc: 0.9119919782303475, aucpr: 0.7919840764583491; Epoch 69 valid loss: 0.4391663082189797, auc: 0.8611033890953717, aucpr: 0.7309298570296748\n",
      "Epoch 70 train loss: 0.33491575226139847, auc: 0.9095116338570755, aucpr: 0.7868140399441302; Epoch 70 valid loss: 0.4323658872592787, auc: 0.8565638950729906, aucpr: 0.7119376698348007\n",
      "Epoch 71 train loss: 0.3490229678234153, auc: 0.8987720440802146, aucpr: 0.7709886448110619; Epoch 71 valid loss: 0.44733816542325144, auc: 0.8395361495459844, aucpr: 0.6892886541351068\n",
      "Epoch 72 train loss: 0.3380666111744547, auc: 0.9070513188492537, aucpr: 0.782098448224463; Epoch 72 valid loss: 0.43383171808238213, auc: 0.8558230098418713, aucpr: 0.7158945915428031\n",
      "Epoch 73 train loss: 0.34564334685555936, auc: 0.9022265462794782, aucpr: 0.7726660454184904; Epoch 73 valid loss: 0.44456912724856795, auc: 0.849162362454028, aucpr: 0.7066055353246214\n",
      "Epoch 74 train loss: 0.34485367369944897, auc: 0.902468827607998, aucpr: 0.7702625749471959; Epoch 74 valid loss: 0.422604041009127, auc: 0.8664878792904882, aucpr: 0.7335343962921266\n",
      "Epoch 75 train loss: 0.3247837497567035, auc: 0.9160653147874716, aucpr: 0.8047148323228361; Epoch 75 valid loss: 0.4408655930100591, auc: 0.8538753876906138, aucpr: 0.7118166019441083\n",
      "Epoch 76 train loss: 0.33468589565138773, auc: 0.9093647113578102, aucpr: 0.7880582285054509; Epoch 76 valid loss: 0.42662606379404766, auc: 0.8663877396712525, aucpr: 0.7260067135154118\n",
      "Epoch 77 train loss: 0.33886601967518154, auc: 0.9065059824980198, aucpr: 0.782326593556498; Epoch 77 valid loss: 0.4362792619097456, auc: 0.857223329260781, aucpr: 0.7112779868026307\n",
      "Epoch 78 train loss: 0.3314674055864081, auc: 0.9117325489650785, aucpr: 0.7937551480676974; Epoch 78 valid loss: 0.4291045590982917, auc: 0.8580919197605994, aucpr: 0.7214253862271105\n",
      "Epoch 79 train loss: 0.3303711100202035, auc: 0.9118090019143938, aucpr: 0.7930190764372191; Epoch 79 valid loss: 0.44982478217053734, auc: 0.8501982157082153, aucpr: 0.7138810336912585\n",
      "Epoch 80 train loss: 0.3328385109279158, auc: 0.9106842954317755, aucpr: 0.7914478333804313; Epoch 80 valid loss: 0.43755713653924344, auc: 0.856258453660508, aucpr: 0.7183928434832625\n",
      "Epoch 81 train loss: 0.3325705279006009, auc: 0.9104682617932067, aucpr: 0.7918259628265558; Epoch 81 valid loss: 0.44719966889780566, auc: 0.849272547182814, aucpr: 0.7030120211664979\n",
      "Epoch 82 train loss: 0.3501122102658438, auc: 0.8992174605386822, aucpr: 0.7659788337772746; Epoch 82 valid loss: 0.4427377732668066, auc: 0.8506471698098514, aucpr: 0.7013688995497108\n",
      "Epoch 83 train loss: 0.3518604614485276, auc: 0.8987538333869004, aucpr: 0.7640201845468063; Epoch 83 valid loss: 0.44387148861198605, auc: 0.8555274967353395, aucpr: 0.7019702019686282\n",
      "Epoch 84 train loss: 0.3477087789950655, auc: 0.9019930770407287, aucpr: 0.7690041432195555; Epoch 84 valid loss: 0.45068744798869065, auc: 0.8484683544008091, aucpr: 0.7044342960162747\n",
      "Epoch 85 train loss: 0.3449039585659543, auc: 0.9020956833903149, aucpr: 0.7708498099335274; Epoch 85 valid loss: 0.436637868920246, auc: 0.8543528418705084, aucpr: 0.7060680519496295\n",
      "Epoch 86 train loss: 0.3395571403753372, auc: 0.9066576354625955, aucpr: 0.781630747776159; Epoch 86 valid loss: 0.4219626800577173, auc: 0.8691629931934648, aucpr: 0.7441003960673985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 train loss: 0.3330614386083503, auc: 0.9104600697779431, aucpr: 0.7885569295896263; Epoch 87 valid loss: 0.4375798679945292, auc: 0.853578784417154, aucpr: 0.7197602378329442\n",
      "Epoch 88 train loss: 0.34746599062077194, auc: 0.9013733237580192, aucpr: 0.7698637078000756; Epoch 88 valid loss: 0.43996401921526956, auc: 0.856789676430669, aucpr: 0.7195031324548217\n",
      "Epoch 89 train loss: 0.3418560995841543, auc: 0.9054839303088928, aucpr: 0.7751087857310566; Epoch 89 valid loss: 0.43121290211243873, auc: 0.8605196825745383, aucpr: 0.726796131565116\n",
      "Epoch 90 train loss: 0.33926290160101963, auc: 0.9063875338840355, aucpr: 0.7819494031790846; Epoch 90 valid loss: 0.4300221590048961, auc: 0.8590745884425174, aucpr: 0.716835291804806\n",
      "Epoch 91 train loss: 0.34364153096489314, auc: 0.9037495595393432, aucpr: 0.7753943080548551; Epoch 91 valid loss: 0.49004613044977585, auc: 0.8112450329269347, aucpr: 0.6372239279153231\n",
      "Epoch 92 train loss: 0.34170307774925773, auc: 0.905423244511858, aucpr: 0.7765554832550662; Epoch 92 valid loss: 0.43760448134118296, auc: 0.8600015028729792, aucpr: 0.7297494661993387\n",
      "Epoch 93 train loss: 0.3283480919747729, auc: 0.9135209589082761, aucpr: 0.7967912326972917; Epoch 93 valid loss: 0.42382111344896994, auc: 0.8670730653235811, aucpr: 0.7267773485318096\n",
      "Epoch 94 train loss: 0.33159454239446806, auc: 0.9112839586200886, aucpr: 0.7904039550326469; Epoch 94 valid loss: 0.42167085133411797, auc: 0.8669622576422648, aucpr: 0.734897238234588\n",
      "Epoch 95 train loss: 0.3316174093644562, auc: 0.911429156736898, aucpr: 0.7928301669744441; Epoch 95 valid loss: 0.4413631693957704, auc: 0.8557151222505407, aucpr: 0.7127501611249683\n",
      "Epoch 96 train loss: 0.32945524379076513, auc: 0.9122751125604631, aucpr: 0.7971416226776149; Epoch 96 valid loss: 0.4119784773278137, auc: 0.8763466870995097, aucpr: 0.7474182853333966\n",
      "Best model found! Loss: 0.4119784773278137\n",
      "Epoch 97 train loss: 0.32568303559628176, auc: 0.9146370163104015, aucpr: 0.798498489769702; Epoch 97 valid loss: 0.4239447538660809, auc: 0.8685331103163275, aucpr: 0.7310617652776473\n",
      "Epoch 98 train loss: 0.33194704040961964, auc: 0.911085926319781, aucpr: 0.7947587661944626; Epoch 98 valid loss: 0.4509674373571419, auc: 0.8478577830521092, aucpr: 0.703900240218226\n",
      "Epoch 99 train loss: 0.3339942149026851, auc: 0.9100625093274867, aucpr: 0.789488427038719; Epoch 99 valid loss: 0.44153199499399515, auc: 0.8514940348401777, aucpr: 0.7074864493572031\n",
      "Epoch 100 train loss: 0.33938678138507544, auc: 0.9068573138369805, aucpr: 0.7815274261084112; Epoch 100 valid loss: 0.4509893863343435, auc: 0.8450253736352473, aucpr: 0.7052323602835344\n",
      "Epoch 101 train loss: 0.3312308144510047, auc: 0.9115862721104026, aucpr: 0.7915892552688463; Epoch 101 valid loss: 0.4451807916625593, auc: 0.8474742779006811, aucpr: 0.7019305768417016\n",
      "Epoch 102 train loss: 0.3283908494404184, auc: 0.9134400983159454, aucpr: 0.7990585751796311; Epoch 102 valid loss: 0.4244520750261554, auc: 0.8645635789245814, aucpr: 0.7377032256968944\n",
      "Epoch 103 train loss: 0.3228561949648046, auc: 0.9166485571882634, aucpr: 0.805509837390255; Epoch 103 valid loss: 0.4155346165237945, auc: 0.874642834060241, aucpr: 0.7521060928458921\n",
      "Epoch 104 train loss: 0.3223767905922466, auc: 0.9163326899559576, aucpr: 0.8057733386236571; Epoch 104 valid loss: 0.4430345481505776, auc: 0.8555441607155233, aucpr: 0.719783492107472\n",
      "Epoch 105 train loss: 0.35991623315963606, auc: 0.8918585201695599, aucpr: 0.7572395329860506; Epoch 105 valid loss: 0.5151109335767231, auc: 0.7424331513533254, aucpr: 0.5206051384031158\n",
      "Epoch 106 train loss: 0.4173618763760969, auc: 0.840193458387829, aucpr: 0.639631886703175; Epoch 106 valid loss: 0.448999573269143, auc: 0.839761969838196, aucpr: 0.6906719963092663\n",
      "Epoch 107 train loss: 0.36412895773430354, auc: 0.8903300349119846, aucpr: 0.7442160522962273; Epoch 107 valid loss: 0.42760303629319724, auc: 0.8553047522712459, aucpr: 0.7106041502109555\n",
      "Epoch 108 train loss: 0.3486765024918593, auc: 0.9004258674909473, aucpr: 0.7647072535407287; Epoch 108 valid loss: 0.43217458307629836, auc: 0.8521738317888005, aucpr: 0.7101985484131779\n",
      "Epoch 109 train loss: 0.34583221311560747, auc: 0.9021433747909182, aucpr: 0.7741566441343597; Epoch 109 valid loss: 0.43303949704586553, auc: 0.8544778995909538, aucpr: 0.7169585054110093\n",
      "Epoch 110 train loss: 0.33368821335101745, auc: 0.9095843296023556, aucpr: 0.7912046986571603; Epoch 110 valid loss: 0.42036917390780476, auc: 0.8669638150235904, aucpr: 0.7409320805463646\n",
      "Epoch 111 train loss: 0.3519068529493272, auc: 0.8983296496859029, aucpr: 0.7585758136499341; Epoch 111 valid loss: 0.4277516198527863, auc: 0.8662730774711553, aucpr: 0.7398283508967787\n",
      "Epoch 112 train loss: 0.3568883566675263, auc: 0.894395575790128, aucpr: 0.7573702555861147; Epoch 112 valid loss: 0.444291177109794, auc: 0.8435151809638166, aucpr: 0.6995173714276534\n",
      "Epoch 113 train loss: 0.33902169053086917, auc: 0.9065632626796574, aucpr: 0.7800518110327401; Epoch 113 valid loss: 0.41933771228862937, auc: 0.8706205463760904, aucpr: 0.7391815248873828\n",
      "Epoch 114 train loss: 0.3398041838637742, auc: 0.9066351513692006, aucpr: 0.7828192605853788; Epoch 114 valid loss: 0.4371331166787186, auc: 0.8549016630496487, aucpr: 0.7318670058751878\n",
      "Epoch 115 train loss: 0.3354417540481608, auc: 0.9090085984133738, aucpr: 0.7899135539366677; Epoch 115 valid loss: 0.41985854038399434, auc: 0.8674107055949702, aucpr: 0.7491307363949318\n",
      "Epoch 116 train loss: 0.3455857192649834, auc: 0.9017385939883431, aucpr: 0.771975152280078; Epoch 116 valid loss: 0.45767103262112235, auc: 0.8297011307367115, aucpr: 0.6629581738556466\n",
      "Epoch 117 train loss: 0.3456518715272187, auc: 0.9021414378571494, aucpr: 0.7704486047482587; Epoch 117 valid loss: 0.43599349143038174, auc: 0.8567677952230442, aucpr: 0.717382277238855\n",
      "Epoch 118 train loss: 0.35486602945870127, auc: 0.8961147755099514, aucpr: 0.7560126091670389; Epoch 118 valid loss: 0.43279963103269276, auc: 0.8538154674441113, aucpr: 0.7144065228878547\n",
      "Epoch 119 train loss: 0.33722914662178016, auc: 0.9081411301595586, aucpr: 0.7850369549864675; Epoch 119 valid loss: 0.4216643180626063, auc: 0.8665609204746587, aucpr: 0.7307218831479961\n",
      "Epoch 120 train loss: 0.35460416867273803, auc: 0.8953543276412583, aucpr: 0.7566398462418178; Epoch 120 valid loss: 0.4442487201953109, auc: 0.8437600013082003, aucpr: 0.6963272979024339\n",
      "Epoch 121 train loss: 0.34536996874927106, auc: 0.9028069120459599, aucpr: 0.7730067484447555; Epoch 121 valid loss: 0.44187590589833614, auc: 0.8418317685544464, aucpr: 0.6879121047960846\n",
      "Epoch 122 train loss: 0.3390369751392133, auc: 0.9067574610656817, aucpr: 0.7828333788530928; Epoch 122 valid loss: 0.41387585690638445, auc: 0.8732846807407216, aucpr: 0.7520175055596852\n",
      "Epoch 123 train loss: 0.3468707102154423, auc: 0.9010061006126246, aucpr: 0.7685488182759026; Epoch 123 valid loss: 0.42671309148838626, auc: 0.8596932581741105, aucpr: 0.7250971582683038\n",
      "Epoch 124 train loss: 0.33398673111838656, auc: 0.9096048751638515, aucpr: 0.7880386600240664; Epoch 124 valid loss: 0.42298664436443445, auc: 0.8689129556216405, aucpr: 0.7418173400370189\n",
      "Epoch 125 train loss: 0.3361896282454168, auc: 0.9075160694865465, aucpr: 0.784777000916613; Epoch 125 valid loss: 0.43296182275047684, auc: 0.8577184207841881, aucpr: 0.7217621736361897\n",
      "Epoch 126 train loss: 0.33363832545933686, auc: 0.9100490578658261, aucpr: 0.7895883637704567; Epoch 126 valid loss: 0.4206047771570914, auc: 0.8656522274057064, aucpr: 0.7396555588236976\n",
      "Epoch 127 train loss: 0.3465841343948486, auc: 0.9020893563929625, aucpr: 0.768450205785312; Epoch 127 valid loss: 0.42685749586006655, auc: 0.866416823767508, aucpr: 0.7291434075828741\n",
      "Epoch 128 train loss: 0.3316384184444533, auc: 0.9109878011271983, aucpr: 0.7934107143206338; Epoch 128 valid loss: 0.421004687944895, auc: 0.8677805447252662, aucpr: 0.736829414366604\n",
      "Epoch 129 train loss: 0.3397212143275655, auc: 0.9051720472151066, aucpr: 0.7825199010272261; Epoch 129 valid loss: 0.46904392644750836, auc: 0.8209553444265605, aucpr: 0.6544570831123577\n",
      "Epoch 130 train loss: 0.3480665010927328, auc: 0.901247766661028, aucpr: 0.7704913275819707; Epoch 130 valid loss: 0.41303967629799737, auc: 0.8715929752757927, aucpr: 0.7396782117086297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131 train loss: 0.3305688776525653, auc: 0.911378809243948, aucpr: 0.7930710748845441; Epoch 131 valid loss: 0.45188016669665493, auc: 0.8401102781716655, aucpr: 0.6752971495849535\n",
      "Epoch 132 train loss: 0.34386417888977566, auc: 0.9038725756093603, aucpr: 0.7731252785237546; Epoch 132 valid loss: 0.4202009552582716, auc: 0.8648005344932709, aucpr: 0.7273479811699557\n",
      "Epoch 133 train loss: 0.3512032060230648, auc: 0.8986551840076213, aucpr: 0.7626083803440579; Epoch 133 valid loss: 0.4383438611015309, auc: 0.8510562160150195, aucpr: 0.700195573897281\n",
      "Epoch 134 train loss: 0.34069498323564157, auc: 0.9056407020844106, aucpr: 0.778845741129393; Epoch 134 valid loss: 0.4287880313963024, auc: 0.8593435481974478, aucpr: 0.7228439801275485\n",
      "Epoch 135 train loss: 0.37252588649239543, auc: 0.8832569408998806, aucpr: 0.736359670822486; Epoch 135 valid loss: 0.4456812254398978, auc: 0.8433160697613391, aucpr: 0.6967934154182498\n",
      "Epoch 136 train loss: 0.3563298695285681, auc: 0.8950978341411158, aucpr: 0.7540970540618221; Epoch 136 valid loss: 0.4292706675417325, auc: 0.8661188967199213, aucpr: 0.7339012264746367\n",
      "Epoch 137 train loss: 0.3421289194746536, auc: 0.9045514788860092, aucpr: 0.7749073002652375; Epoch 137 valid loss: 0.44839114179002076, auc: 0.8452276385349092, aucpr: 0.6947799241473054\n",
      "Epoch 138 train loss: 0.3458740400856485, auc: 0.9023634644838645, aucpr: 0.7738925697521952; Epoch 138 valid loss: 0.4701011995719812, auc: 0.8211464351152112, aucpr: 0.648500092947135\n",
      "Epoch 139 train loss: 0.3391785836328152, auc: 0.9064410185065137, aucpr: 0.780424461120441; Epoch 139 valid loss: 0.44490107141286694, auc: 0.8521860572322062, aucpr: 0.6997308172512156\n",
      "Epoch 140 train loss: 0.34518552893501986, auc: 0.9030598912578535, aucpr: 0.7693123178616749; Epoch 140 valid loss: 0.4483291354677098, auc: 0.8427490272206896, aucpr: 0.6966107971776848\n",
      "Epoch 141 train loss: 0.34865742136468053, auc: 0.900244639529402, aucpr: 0.77000855933683; Epoch 141 valid loss: 0.41799429794595266, auc: 0.8668804172536047, aucpr: 0.737160664624661\n",
      "Epoch 142 train loss: 0.3491177025810098, auc: 0.9003774633242869, aucpr: 0.7701700195195789; Epoch 142 valid loss: 0.4378265277094214, auc: 0.8503499435838614, aucpr: 0.7072453463192835\n",
      "Epoch 143 train loss: 0.34859179601890034, auc: 0.9011104553166192, aucpr: 0.7733272256359289; Epoch 143 valid loss: 0.4275456142130048, auc: 0.8610760959876408, aucpr: 0.7298821416555735\n",
      "Epoch 144 train loss: 0.3385026941561345, auc: 0.9067048570129281, aucpr: 0.7836934572684451; Epoch 144 valid loss: 0.42378371065105364, auc: 0.8593419129470561, aucpr: 0.7206562850455255\n",
      "Epoch 145 train loss: 0.36546030489307685, auc: 0.8872654173439596, aucpr: 0.7410995145130568; Epoch 145 valid loss: 0.42542680157922136, auc: 0.8580702332256405, aucpr: 0.7251406862624284\n",
      "Epoch 146 train loss: 0.357762886425353, auc: 0.8941929344824034, aucpr: 0.757550608873798; Epoch 146 valid loss: 0.4349723095968037, auc: 0.8534182573370179, aucpr: 0.7120476855086908\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.4119784773278137, roc_auc 0.8763466870995097, aucpr 0.7474182853333966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4119784773278137,\n",
       " 0.8763466870995097,\n",
       " 0.7474182853333966,\n",
       " 0.7171139463264409,\n",
       " 0.6404715127701375,\n",
       " 0.6766293067662931)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def test(test_dataloader):\n",
    "    best_model = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        threshold = 0.5\n",
    "        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n",
    "        \n",
    "        return valid_loss, roc_auc, aucpr, p, r, f1\n",
    "        \n",
    "test(test_dataloader1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import itertools\n",
    "\n",
    "n_epochs = 600\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "hidden_dims = [32, 64, 128]\n",
    "num_layers = [1, 2]\n",
    "dropout = 0\n",
    "\n",
    "early_stopping_patience = 50\n",
    "rnn = 'GRU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning_rate = 0.1, hidden_dim = 128, num_layer = 2.\n",
      "valid loss 1.097691842581795, roc_auc 0.4998190383611922, aucpr 0.45540833132492897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmnlab/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "f_out = 'lte_ho_cls_rnn.csv'\n",
    "f_out = open(f_out, 'w')\n",
    "cols_out = ['lr','hidden_dim','num_layer', 'valid_loss','auc','aucpr', 'p', 'r', 'f1']\n",
    "f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "for lr, hidden_dim, num_layer in itertools.product(lrs, hidden_dims, num_layers):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # For record loss\n",
    "    train_losses_for_epochs = []\n",
    "    validation_losses_for_epochs = []\n",
    "    valid_losses_for_epochs = []\n",
    "\n",
    "    # Save best model to ... \n",
    "    best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "    \n",
    "    train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f'For learning_rate = {lr}, hidden_dim = {hidden_dim}, num_layer = {num_layer}.')\n",
    "    valid_loss, roc_auc, aucpr, p, r, f1 = test(test_dataloader1)\n",
    "    \n",
    "    cols_out = [lr, hidden_dim, num_layer, valid_loss, roc_auc, aucpr, p, r, f1]\n",
    "    cols_out = [str(n) for n in cols_out]\n",
    "    f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "forecaster = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(forecaster.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fst(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    def rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "    def mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        forecaster.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = forecaster(features)\n",
    "            \n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "        \n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, rmse: {rmse_error}, mae: {mae_error}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        forecaster.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = forecaster(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, rmse: {rmse_error}, mae: {mae_error}')\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(forecaster.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_fst_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 7.84267113002894, rmse: 2.799887180328369, mae: 2.3972580432891846; Epoch 1 valid loss: 6.742581599950791, rmse: 2.596402406692505, mae: 2.2201855182647705\n",
      "Best model found! Loss: 6.742581599950791\n",
      "Epoch 2 train loss: 6.454146884610062, rmse: 2.5395233631134033, mae: 2.138516664505005; Epoch 2 valid loss: 6.255369633436203, rmse: 2.5016636848449707, mae: 2.119319438934326\n",
      "Best model found! Loss: 6.255369633436203\n",
      "Epoch 3 train loss: 6.09969925946844, rmse: 2.468736171722412, mae: 2.0514492988586426; Epoch 3 valid loss: 6.035796386003494, rmse: 2.4569129943847656, mae: 2.0519251823425293\n",
      "Best model found! Loss: 6.035796386003494\n",
      "Epoch 4 train loss: 6.057478280452632, rmse: 2.4601759910583496, mae: 2.0372040271759033; Epoch 4 valid loss: 5.957492783665657, rmse: 2.440838575363159, mae: 2.0111448764801025\n",
      "Best model found! Loss: 5.957492783665657\n",
      "Epoch 5 train loss: 5.926748684853896, rmse: 2.433544874191284, mae: 2.006577730178833; Epoch 5 valid loss: 5.8642287939786915, rmse: 2.421943426132202, mae: 1.9750311374664307\n",
      "Best model found! Loss: 5.8642287939786915\n",
      "Epoch 6 train loss: 5.903473055794379, rmse: 2.4285054206848145, mae: 1.997105360031128; Epoch 6 valid loss: 5.936354812979698, rmse: 2.4360032081604004, mae: 1.9666610956192017\n",
      "Epoch 7 train loss: 5.8404380777088045, rmse: 2.4156055450439453, mae: 1.9833166599273682; Epoch 7 valid loss: 5.907692614197731, rmse: 2.4301278591156006, mae: 1.9680192470550537\n",
      "Epoch 8 train loss: 5.827346053960263, rmse: 2.412883758544922, mae: 1.9815014600753784; Epoch 8 valid loss: 5.862418267130852, rmse: 2.42187237739563, mae: 1.9611127376556396\n",
      "Best model found! Loss: 5.862418267130852\n",
      "Epoch 9 train loss: 5.832153281129503, rmse: 2.4138333797454834, mae: 1.981968641281128; Epoch 9 valid loss: 5.867449027299881, rmse: 2.422950029373169, mae: 1.9528355598449707\n",
      "Epoch 10 train loss: 5.752449316566701, rmse: 2.3972115516662598, mae: 1.963720679283142; Epoch 10 valid loss: 5.829962635040284, rmse: 2.414222002029419, mae: 1.9345940351486206\n",
      "Best model found! Loss: 5.829962635040284\n",
      "Epoch 11 train loss: 5.731683568370044, rmse: 2.3927576541900635, mae: 1.9580110311508179; Epoch 11 valid loss: 5.76096111536026, rmse: 2.400707721710205, mae: 1.9317560195922852\n",
      "Best model found! Loss: 5.76096111536026\n",
      "Epoch 12 train loss: 5.666357234327906, rmse: 2.3791093826293945, mae: 1.9434914588928223; Epoch 12 valid loss: 5.763573896884918, rmse: 2.4018824100494385, mae: 1.9263685941696167\n",
      "Epoch 13 train loss: 5.633378050785542, rmse: 2.3718204498291016, mae: 1.9345033168792725; Epoch 13 valid loss: 5.943169412016869, rmse: 2.43747615814209, mae: 1.94707190990448\n",
      "Epoch 14 train loss: 5.576580379666725, rmse: 2.360008478164673, mae: 1.9207675457000732; Epoch 14 valid loss: 5.7101508498191835, rmse: 2.390003204345703, mae: 1.9208523035049438\n",
      "Best model found! Loss: 5.7101508498191835\n",
      "Epoch 15 train loss: 5.549513227760293, rmse: 2.3541195392608643, mae: 1.912928819656372; Epoch 15 valid loss: 5.595260626077652, rmse: 2.3651134967803955, mae: 1.9040275812149048\n",
      "Best model found! Loss: 5.595260626077652\n",
      "Epoch 16 train loss: 5.47432933112705, rmse: 2.337926149368286, mae: 1.892931580543518; Epoch 16 valid loss: 5.653804138302803, rmse: 2.3777666091918945, mae: 1.917594313621521\n",
      "Epoch 17 train loss: 5.4286050759982265, rmse: 2.3280889987945557, mae: 1.8848787546157837; Epoch 17 valid loss: 5.811259371042252, rmse: 2.4100801944732666, mae: 1.9257222414016724\n",
      "Epoch 18 train loss: 5.433602198252771, rmse: 2.329364538192749, mae: 1.8855098485946655; Epoch 18 valid loss: 5.702516093850136, rmse: 2.3874967098236084, mae: 1.963572382926941\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_fst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 36\u001b[0m, in \u001b[0;36mtrain_fst\u001b[0;34m(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m     33\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((preds, out\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# metrics calculate\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_fst(n_epochs, train_dataloader2, test_dataloader2, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 5.981105654580253, rmse 2.443638801574707, mae 1.988729476928711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.981105654580253, 2.443638801574707, 1.988729476928711)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def rmse(predictions, targets):\n",
    "    return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "def mae(predictions, targets):\n",
    "    return torch.mean(torch.abs(predictions - targets))\n",
    "\n",
    "def test2(test_dataloader):\n",
    "    best_model = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, rmse {rmse_error}, mae {mae_error}')\n",
    "        \n",
    "        return valid_loss, rmse_error.item(), mae_error.item()\n",
    "        \n",
    "test2(test_dataloader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = \"/home/wmnlab/Documents/YU/model\"\n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "torch.save(classifier.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# m_path = os.path.join('/home/wmnlab/Documents/sheng-ru/model', 'lte_HO_cls_RNN.pt')\n",
    "# classifier = RNN(input_dim, out_dim, hidden_dim, num_layers, dropout, rnn)\n",
    "# classifier.load_state_dict(torch.load(m_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
