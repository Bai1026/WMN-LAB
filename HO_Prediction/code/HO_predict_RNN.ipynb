{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 20 12:26:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 Ti      On | 00000000:65:00.0 Off |                  N/A |\n",
      "|  0%   36C    P8               10W / 285W|     70MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4070 Ti      On | 00000000:B3:00.0 Off |                  N/A |\n",
      "|  0%   37C    P8                9W / 285W|      8MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                           56MiB |\n",
      "|    0   N/A  N/A      1400      G   /usr/bin/gnome-shell                          9MiB |\n",
      "|    1   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Dataset_simple(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset take all csv file specified in dir_list in directory dirname.\n",
    "    Transfer csvs to (features, label) pair\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.inputs = torch.FloatTensor(X)\n",
    "        self.labels = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_file(file, dates):\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in file: return True \n",
    "    return False\n",
    "\n",
    "def train_valid_split(L, valid_size=0.2):\n",
    "    \n",
    "    length = len(L)\n",
    "    v_num = int(length*valid_size)\n",
    "    v_files = random.sample(L, v_num)\n",
    "    t_files = list(set(L) - set(v_files))\n",
    "    \n",
    "    return t_files, v_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n",
    "    \n",
    "    print(f\"Random seed set as {seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time sequence length and prediction time length\n",
    "seed = 55688\n",
    "time_seq = 20\n",
    "predict_t = 10\n",
    "valid_ratio = 0.2\n",
    "task = 'classification'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheng Ru's Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_array_create(dirname, dir_list, time_seq):\n",
    "    \n",
    "    columns = ['RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "               'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2']\n",
    "    \n",
    "    def reamin_HO_time(y_train):\n",
    "        def f(L):    \n",
    "            for i, e in enumerate(L):\n",
    "                if e: return i+1\n",
    "            return 0\n",
    "\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            a1_out = []\n",
    "            for a1 in a2:\n",
    "                a1_out.append(a1.any())\n",
    "      \n",
    "            out.append(f(a1_out))\n",
    "        return out\n",
    "    \n",
    "    def HO(y_train):\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            if sum(a2.reshape(-1)) == 0: ho = 0\n",
    "            elif sum(a2.reshape(-1)) > 0: ho = 1\n",
    "            out.append(ho)\n",
    "        return out\n",
    "\n",
    "    split_time = []\n",
    "    for i, f in enumerate(tqdm(dir_list)):\n",
    "    \n",
    "        f = os.path.join(dirname, f)\n",
    "        # df = pd.read_csv(f)\n",
    "\n",
    "        # try:\n",
    "        #     df = pd.read_csv(f)\n",
    "        # except pd.errors.ParserError as e:\n",
    "        #     print(f\"ParserError occurred while reading the file {f}: {e}\")\n",
    "        #     print(f, '\\n')\n",
    "        # except KeyError as e:\n",
    "        #     print(f\"KeyError: The key {e} was not found in the file {f}\")\n",
    "        #     print(f, '\\n')\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            # print(df.dtypes)\n",
    "            # break\n",
    "            if 'Timestamp' in df.columns:\n",
    "                del df['Timestamp']\n",
    "            else:\n",
    "                print(f\"Warning: 'Timestamp' not found in {f}\")\n",
    "\n",
    "            del df['lat'], df['long'], df['gpsspeed']\n",
    "            # df['lat'], df['long'], df['gpsspeed'] = df['lat'].replace('-', 0), df['long'].replace('-', 0), df['gpsspeed'].replace('-', 0)\n",
    "            # df.fillna(0, inplace=True)\n",
    "\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"ParserError occurred while reading the file {f}: {e}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: The key {e} was not found in the file {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing {f}: {e}\")\n",
    "\n",
    "\n",
    "        # preprocess data with ffill method\n",
    "        # del df['Timestamp'], df['lat'], df['long'], df['gpsspeed']\n",
    "        # df[columns] = df[columns].replace(0, np.nan)\n",
    "        # df[columns] = df[columns].fillna(method='ffill')\n",
    "        # df.dropna(inplace=True)\n",
    "        \n",
    "        df.replace(np.nan,0,inplace=True); df.replace('-',0,inplace=True)\n",
    "        \n",
    "        X = df[features]\n",
    "        Y = df[target]\n",
    "\n",
    "        Xt_list = []\n",
    "        Yt_list = []\n",
    "\n",
    "        for j in range(time_seq):\n",
    "            X_t = X.shift(periods=-j)\n",
    "            Xt_list.append(X_t)\n",
    "    \n",
    "        for j in range(time_seq,time_seq+predict_t):\n",
    "            Y_t = Y.shift(periods=-(j))\n",
    "            Yt_list.append(Y_t)\n",
    "\n",
    "        # YY = Y.shift(periods=-(0))\n",
    "\n",
    "        X_ts = np.array(Xt_list); X_ts = np.transpose(X_ts, (1,0,2)); X_ts = X_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        Y_ts = np.array(Yt_list); Y_ts = np.transpose(Y_ts, (1,0,2)); Y_ts = Y_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        split_time.append(len(X_ts))\n",
    "\n",
    "        if i == 0:\n",
    "            X_final = X_ts\n",
    "            Y_final = Y_ts\n",
    "        else:\n",
    "            X_final = np.concatenate((X_final,X_ts), axis=0)\n",
    "            Y_final = np.concatenate((Y_final,Y_ts), axis=0)\n",
    "        # Y_final = np.array([np.array(pd.to_numeric(df[col], errors='coerce').fillna(0)) for col in target_columns])\n",
    "\n",
    "    split_time = [(sum(split_time[:i]), sum(split_time[:i])+x) for i, x in enumerate(split_time)]\n",
    "    \n",
    "    return X_final, np.array(HO(Y_final)), np.array(reamin_HO_time(Y_final)), split_time # forecast HO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Ti\n",
      "GPU 1: NVIDIA GeForce RTX 4070 Ti\n",
      "Loading training data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ts_array_create() missing 3 required positional arguments: 'predict_t', 'features', and 'target_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 52\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# target = ['RLF'] # For RLF\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# target = ['SCG_RLF'] # For scg failure\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# target = ['dl-loss'] # For DL loss\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# target = ['ul-loss'] # For UL loss\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Data\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading training data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m X_train, y_train1, y_train2, split_time_train \u001b[38;5;241m=\u001b[39m \u001b[43mts_array_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m RNN_Dataset_simple(X_train, y_train1)\n\u001b[1;32m     55\u001b[0m train_dataloader1 \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: ts_array_create() missing 3 required positional arguments: 'predict_t', 'features', and 'target_columns'"
     ]
    }
   ],
   "source": [
    "# Setup seed\n",
    "set_seed(seed)\n",
    "\n",
    "# Get GPU\n",
    "device_count = torch.cuda.device_count()\n",
    "num_of_gpus = device_count\n",
    "\n",
    "for i in range(device_count):\n",
    "    print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "    gpu_id = i\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Save best model to \n",
    "save_path = \"../model\"\n",
    "\n",
    "# Define DataSet\n",
    "dirname = \"../data/single\"\n",
    "# dirname = \"/home/wmnlab/Documents/sheng-ru/data/single0.5\"\n",
    "dir_list = os.listdir(dirname)\n",
    "dir_list = [f for f in dir_list if ( f.endswith('.csv') and (not 'sm' in f) ) ]\n",
    "\n",
    "train_dates = ['03-26', '04-01']\n",
    "test_dates = ['04-10']\n",
    "    \n",
    "# train_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, train_dates) )]\n",
    "# test_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, test_dates) )]\n",
    "\n",
    "train_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio)\n",
    "train_dir_list += [f for f in os.listdir(dirname) if 'sm' in f]\n",
    "\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "#         'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2' ]\n",
    "features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "        'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1','nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2']\n",
    "\n",
    "num_of_features = len(features)\n",
    "\n",
    "# target = ['LTE_HO', 'MN_HO'] # For eNB HO.\n",
    "# target = ['eNB_to_ENDC'] # Setup gNB\n",
    "target = ['gNB_Rel', 'gNB_HO'] # For gNB HO.\n",
    "# target = ['RLF'] # For RLF\n",
    "# target = ['SCG_RLF'] # For scg failure\n",
    "# target = ['dl-loss'] # For DL loss\n",
    "# target = ['ul-loss'] # For UL loss\n",
    "\n",
    "# Data\n",
    "print('Loading training data...')\n",
    "X_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq)\n",
    "\n",
    "train_dataset = RNN_Dataset_simple(X_train, y_train1)\n",
    "train_dataloader1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_train2 > 0\n",
    "X_train_fore = X_train[cond]\n",
    "y_train2_fore = y_train2[cond]\n",
    "train_dataset = RNN_Dataset_simple(X_train_fore, y_train2_fore)\n",
    "train_dataloader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Loading testing data...')\n",
    "X_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq)\n",
    "\n",
    "test_dataset = RNN_Dataset_simple(X_test, y_test1)\n",
    "test_dataloader1 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_test2 > 0\n",
    "X_test_fore = X_test[cond]\n",
    "y_test2_fore = y_test2[cond]\n",
    "test_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\n",
    "test_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_array_create(dirname, dir_list, time_seq, predict_t, features, target_columns):\n",
    "    split_time = []\n",
    "    X_final, Y_final = None, None\n",
    "\n",
    "    for i, f in enumerate(tqdm(dir_list)):\n",
    "        f_path = os.path.join(dirname, f)\n",
    "        try:\n",
    "            df = pd.read_csv(f_path)\n",
    "            \n",
    "            # Convert all columns that should be numeric but contain '-' to numeric,\n",
    "            # coercing errors by converting them to NaN and then filling with 0.\n",
    "            for col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.fillna(0, inplace=True)\n",
    "\n",
    "            if 'Timestamp' in df.columns:\n",
    "                df.drop('Timestamp', axis=1, inplace=True)\n",
    "            if {'lat', 'long', 'gpsspeed'}.issubset(df.columns):\n",
    "                df.drop(['lat', 'long', 'gpsspeed'], axis=1, inplace=True)\n",
    "\n",
    "            X = df[features]\n",
    "            Y = df[target_columns]\n",
    "\n",
    "            Xt_list = [X.shift(-j) for j in range(time_seq)]\n",
    "            Yt_list = [Y.shift(-j) for j in range(time_seq, time_seq + predict_t)]\n",
    "\n",
    "            X_ts = np.stack(Xt_list, axis=1)[:-predict_t]\n",
    "            Y_ts = np.stack(Yt_list, axis=1)[:-predict_t]\n",
    "\n",
    "            if X_final is None:\n",
    "                X_final, Y_final = X_ts, Y_ts\n",
    "            else:\n",
    "                X_final = np.concatenate([X_final, X_ts], axis=0)\n",
    "                Y_final = np.concatenate([Y_final, Y_ts], axis=0)\n",
    "\n",
    "            split_time.append(len(X_ts))\n",
    "\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"ParserError occurred while reading the file {f_path}: {e}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: The key {e} was not found in the file {f_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing {f_path}: {e}\")\n",
    "\n",
    "    y_train1 = HO(Y_final)\n",
    "    y_train2 = remain_HO_time(Y_final)\n",
    "\n",
    "    return X_final, y_train1, y_train2, split_time\n",
    "\n",
    "def HO(y_train):\n",
    "    return np.any(y_train > 0, axis=1).astype(int)\n",
    "\n",
    "def remain_HO_time(y_train):\n",
    "    result = np.zeros(y_train.shape[0], dtype=int)\n",
    "    for i in range(y_train.shape[0]):\n",
    "        condition_met_indices = np.where(y_train[i] > 0)[0]\n",
    "        if condition_met_indices.size > 0:\n",
    "            result[i] = condition_met_indices[0] + 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Ti\n",
      "GPU 1: NVIDIA GeForce RTX 4070 Ti\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:02<00:00, 53.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 70.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Function to split data into training and validation sets\n",
    "def train_valid_split(file_list, valid_ratio=0.2):\n",
    "    np.random.shuffle(file_list)\n",
    "    split_idx = int(len(file_list) * (1 - valid_ratio))\n",
    "    return file_list[:split_idx], file_list[split_idx:]\n",
    "\n",
    "# # RNN Dataset\n",
    "# class RNN_Dataset_simple(torch.utils.data.Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = torch.tensor(X, dtype=torch.float32)\n",
    "#         self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]\n",
    "\n",
    "class RNN_Dataset_simple(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset take all csv file specified in dir_list in directory dirname.\n",
    "    Transfer csvs to (features, label) pair\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.inputs = torch.FloatTensor(X)\n",
    "        self.labels = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "# Set random seed\n",
    "set_seed(123)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# List available GPUs\n",
    "if device.type == 'cuda':\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Directory and file setup\n",
    "dirname = \"../data/single\"\n",
    "dir_list = [f for f in os.listdir(dirname) if f.endswith('.csv') and 'sm' not in f]\n",
    "\n",
    "# Splitting data\n",
    "train_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio=0.2)\n",
    "\n",
    "# Define features and targets\n",
    "features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "            'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n",
    "target = ['gNB_Rel', 'gNB_HO']\n",
    "\n",
    "# Define sequence length and prediction time\n",
    "time_seq = 10\n",
    "predict_t = 2\n",
    "\n",
    "# Load training data\n",
    "print('Loading training data...')\n",
    "X_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq, predict_t, features, target)\n",
    "\n",
    "# Create datasets and dataloaders for training\n",
    "train_dataset1 = RNN_Dataset_simple(X_train, y_train1)\n",
    "train_dataloader1 = DataLoader(train_dataset1, batch_size=32, shuffle=True)\n",
    "\n",
    "cond = y_train2 > 0\n",
    "X_train_fore = X_train[cond]\n",
    "y_train2_fore = y_train2[cond]\n",
    "train_dataset2 = RNN_Dataset_simple(X_train_fore, y_train2_fore)\n",
    "train_dataloader2 = DataLoader(train_dataset2, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load testing data\n",
    "print('Loading testing data...')\n",
    "X_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq, predict_t, features, target)\n",
    "\n",
    "# Create datasets and dataloaders for testing\n",
    "test_dataset1 = RNN_Dataset_simple(X_test, y_test1)\n",
    "test_dataloader1 = DataLoader(test_dataset1, batch_size=32, shuffle=False)\n",
    "\n",
    "cond = y_test2 > 0\n",
    "X_test_fore = X_test[cond]\n",
    "y_test2_fore = y_test2[cond]\n",
    "test_dataset2 = RNN_Dataset_simple(X_test_fore, y_test2_fore)\n",
    "test_dataloader2 = DataLoader(test_dataset2, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 16])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original is [32, 20, 16]\n",
    "a,b = next(iter(train_dataloader1))\n",
    "input_dim, out_dim = a.shape[2], 1\n",
    "print(input_dim, out_dim)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN_Cls(nn.Module):\n",
    "#     '''\n",
    "#     Using LSTM or GRU.\n",
    "#     '''\n",
    "#     def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "#         super().__init__()\n",
    "#         self.in_dim = input_dim\n",
    "#         self.out_dim = out_dim\n",
    "#         self.hid_dim = hidden_dim\n",
    "#         self.num_layer = num_layer\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         # input_size: num of features; hidden_size: num of hidden state h\n",
    "#         # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "#         if rnn == 'LSTM':\n",
    "#             self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "#         elif rnn == 'GRU':\n",
    "#             self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "#         self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "#     def forward(self,batch_input):\n",
    "\n",
    "#         out,_ = self.rnn(batch_input)\n",
    "#         out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "        \n",
    "#         out = torch.sigmoid(out) # Binary Classifier\n",
    "\n",
    "#         return out\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN_Cls(nn.Module):\n",
    "    '''\n",
    "    RNN Classifier using LSTM or GRU for binary classification.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn_type='LSTM'):\n",
    "        super(RNN_Cls, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layer\n",
    "\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layer, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, out_dim)  # Output dimension for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        # rnn_out shape: (batch_size, sequence_length, hidden_dim)\n",
    "        last_rnn_output = rnn_out[:, -1, :]  # get the last sequence output\n",
    "        # last_rnn_output shape: (batch_size, hidden_dim)\n",
    "        final_output = self.output_layer(last_rnn_output)  \n",
    "        # final_output shape: (batch_size, out_dim)\n",
    "        return torch.sigmoid(final_output)  # Apply sigmoid to output layer for binary classification probability\n",
    "\n",
    "\n",
    "class RNN_Fst(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters, input_dim = 16, out_dim = 1\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "# Define model and optimizer\n",
    "\n",
    "classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[600, 1000], gamma=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        classifier.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = classifier(features)\n",
    "\n",
    "            print(\"trues: \", trues.shape)\n",
    "            print(trues)\n",
    "            print(\"labels: \", labels.cpu().numpy().shape)\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            \n",
    "            \n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "        \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, auc: {roc_auc}, aucpr: {aucpr}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        classifier.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = classifier(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            \n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, auc: {roc_auc}, aucpr: {aucpr}')\n",
    "        \n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(classifier.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/lte_HO_cls_RNN.pt\n"
     ]
    }
   ],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "print(best_model_path)\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# train_cls(n_epochs, train_dataloader1, test_dataloader1, classifier, criterion, optimizer, best_model_path, early_stopping_patience)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[119], line 24\u001b[0m, in \u001b[0;36mtrain_cls\u001b[0;34m(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m     20\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrues: \u001b[39m\u001b[38;5;124m\"\u001b[39m, trues\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(trues)\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[85], line 54\u001b[0m, in \u001b[0;36mRNN_Cls.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# x shape: (batch_size, sequence_length, input_dim)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     rnn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# rnn_out shape: (batch_size, sequence_length, hidden_dim)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     last_rnn_output \u001b[38;5;241m=\u001b[39m rnn_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# get the last sequence output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/nn/modules/rnn.py:955\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    959\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "source": [
    "train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)\n",
    "# train_cls(n_epochs, train_dataloader1, test_dataloader1, classifier, criterion, optimizer, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.4119784773278137, roc_auc 0.8763466870995097, aucpr 0.7474182853333966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4119784773278137,\n",
       " 0.8763466870995097,\n",
       " 0.7474182853333966,\n",
       " 0.7171139463264409,\n",
       " 0.6404715127701375,\n",
       " 0.6766293067662931)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def test(test_dataloader):\n",
    "    best_model = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        threshold = 0.5\n",
    "        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n",
    "        \n",
    "        return valid_loss, roc_auc, aucpr, p, r, f1\n",
    "        \n",
    "test(test_dataloader1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import itertools\n",
    "\n",
    "n_epochs = 600\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "hidden_dims = [32, 64, 128]\n",
    "num_layers = [1, 2]\n",
    "dropout = 0\n",
    "\n",
    "early_stopping_patience = 50\n",
    "rnn = 'GRU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning_rate = 0.1, hidden_dim = 128, num_layer = 2.\n",
      "valid loss 1.097691842581795, roc_auc 0.4998190383611922, aucpr 0.45540833132492897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmnlab/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "f_out = 'lte_ho_cls_rnn.csv'\n",
    "f_out = open(f_out, 'w')\n",
    "cols_out = ['lr','hidden_dim','num_layer', 'valid_loss','auc','aucpr', 'p', 'r', 'f1']\n",
    "f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "for lr, hidden_dim, num_layer in itertools.product(lrs, hidden_dims, num_layers):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # For record loss\n",
    "    train_losses_for_epochs = []\n",
    "    validation_losses_for_epochs = []\n",
    "    valid_losses_for_epochs = []\n",
    "\n",
    "    # Save best model to ... \n",
    "    best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "    \n",
    "    train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f'For learning_rate = {lr}, hidden_dim = {hidden_dim}, num_layer = {num_layer}.')\n",
    "    valid_loss, roc_auc, aucpr, p, r, f1 = test(test_dataloader1)\n",
    "    \n",
    "    cols_out = [lr, hidden_dim, num_layer, valid_loss, roc_auc, aucpr, p, r, f1]\n",
    "    cols_out = [str(n) for n in cols_out]\n",
    "    f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "forecaster = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(forecaster.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fst(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    def rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "    def mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        forecaster.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = forecaster(features)\n",
    "            \n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "        \n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, rmse: {rmse_error}, mae: {mae_error}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        forecaster.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = forecaster(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, rmse: {rmse_error}, mae: {mae_error}')\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(forecaster.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_fst_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 7.84267113002894, rmse: 2.799887180328369, mae: 2.3972580432891846; Epoch 1 valid loss: 6.742581599950791, rmse: 2.596402406692505, mae: 2.2201855182647705\n",
      "Best model found! Loss: 6.742581599950791\n",
      "Epoch 2 train loss: 6.454146884610062, rmse: 2.5395233631134033, mae: 2.138516664505005; Epoch 2 valid loss: 6.255369633436203, rmse: 2.5016636848449707, mae: 2.119319438934326\n",
      "Best model found! Loss: 6.255369633436203\n",
      "Epoch 3 train loss: 6.09969925946844, rmse: 2.468736171722412, mae: 2.0514492988586426; Epoch 3 valid loss: 6.035796386003494, rmse: 2.4569129943847656, mae: 2.0519251823425293\n",
      "Best model found! Loss: 6.035796386003494\n",
      "Epoch 4 train loss: 6.057478280452632, rmse: 2.4601759910583496, mae: 2.0372040271759033; Epoch 4 valid loss: 5.957492783665657, rmse: 2.440838575363159, mae: 2.0111448764801025\n",
      "Best model found! Loss: 5.957492783665657\n",
      "Epoch 5 train loss: 5.926748684853896, rmse: 2.433544874191284, mae: 2.006577730178833; Epoch 5 valid loss: 5.8642287939786915, rmse: 2.421943426132202, mae: 1.9750311374664307\n",
      "Best model found! Loss: 5.8642287939786915\n",
      "Epoch 6 train loss: 5.903473055794379, rmse: 2.4285054206848145, mae: 1.997105360031128; Epoch 6 valid loss: 5.936354812979698, rmse: 2.4360032081604004, mae: 1.9666610956192017\n",
      "Epoch 7 train loss: 5.8404380777088045, rmse: 2.4156055450439453, mae: 1.9833166599273682; Epoch 7 valid loss: 5.907692614197731, rmse: 2.4301278591156006, mae: 1.9680192470550537\n",
      "Epoch 8 train loss: 5.827346053960263, rmse: 2.412883758544922, mae: 1.9815014600753784; Epoch 8 valid loss: 5.862418267130852, rmse: 2.42187237739563, mae: 1.9611127376556396\n",
      "Best model found! Loss: 5.862418267130852\n",
      "Epoch 9 train loss: 5.832153281129503, rmse: 2.4138333797454834, mae: 1.981968641281128; Epoch 9 valid loss: 5.867449027299881, rmse: 2.422950029373169, mae: 1.9528355598449707\n",
      "Epoch 10 train loss: 5.752449316566701, rmse: 2.3972115516662598, mae: 1.963720679283142; Epoch 10 valid loss: 5.829962635040284, rmse: 2.414222002029419, mae: 1.9345940351486206\n",
      "Best model found! Loss: 5.829962635040284\n",
      "Epoch 11 train loss: 5.731683568370044, rmse: 2.3927576541900635, mae: 1.9580110311508179; Epoch 11 valid loss: 5.76096111536026, rmse: 2.400707721710205, mae: 1.9317560195922852\n",
      "Best model found! Loss: 5.76096111536026\n",
      "Epoch 12 train loss: 5.666357234327906, rmse: 2.3791093826293945, mae: 1.9434914588928223; Epoch 12 valid loss: 5.763573896884918, rmse: 2.4018824100494385, mae: 1.9263685941696167\n",
      "Epoch 13 train loss: 5.633378050785542, rmse: 2.3718204498291016, mae: 1.9345033168792725; Epoch 13 valid loss: 5.943169412016869, rmse: 2.43747615814209, mae: 1.94707190990448\n",
      "Epoch 14 train loss: 5.576580379666725, rmse: 2.360008478164673, mae: 1.9207675457000732; Epoch 14 valid loss: 5.7101508498191835, rmse: 2.390003204345703, mae: 1.9208523035049438\n",
      "Best model found! Loss: 5.7101508498191835\n",
      "Epoch 15 train loss: 5.549513227760293, rmse: 2.3541195392608643, mae: 1.912928819656372; Epoch 15 valid loss: 5.595260626077652, rmse: 2.3651134967803955, mae: 1.9040275812149048\n",
      "Best model found! Loss: 5.595260626077652\n",
      "Epoch 16 train loss: 5.47432933112705, rmse: 2.337926149368286, mae: 1.892931580543518; Epoch 16 valid loss: 5.653804138302803, rmse: 2.3777666091918945, mae: 1.917594313621521\n",
      "Epoch 17 train loss: 5.4286050759982265, rmse: 2.3280889987945557, mae: 1.8848787546157837; Epoch 17 valid loss: 5.811259371042252, rmse: 2.4100801944732666, mae: 1.9257222414016724\n",
      "Epoch 18 train loss: 5.433602198252771, rmse: 2.329364538192749, mae: 1.8855098485946655; Epoch 18 valid loss: 5.702516093850136, rmse: 2.3874967098236084, mae: 1.963572382926941\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_fst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 36\u001b[0m, in \u001b[0;36mtrain_fst\u001b[0;34m(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m     33\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((preds, out\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# metrics calculate\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_fst(n_epochs, train_dataloader2, test_dataloader2, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 5.981105654580253, rmse 2.443638801574707, mae 1.988729476928711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.981105654580253, 2.443638801574707, 1.988729476928711)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def rmse(predictions, targets):\n",
    "    return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "def mae(predictions, targets):\n",
    "    return torch.mean(torch.abs(predictions - targets))\n",
    "\n",
    "def test2(test_dataloader):\n",
    "    best_model = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, rmse {rmse_error}, mae {mae_error}')\n",
    "        \n",
    "        return valid_loss, rmse_error.item(), mae_error.item()\n",
    "        \n",
    "test2(test_dataloader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = \"/home/wmnlab/Documents/YU/model\"\n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "torch.save(classifier.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# m_path = os.path.join('/home/wmnlab/Documents/sheng-ru/model', 'lte_HO_cls_RNN.pt')\n",
    "# classifier = RNN(input_dim, out_dim, hidden_dim, num_layers, dropout, rnn)\n",
    "# classifier.load_state_dict(torch.load(m_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
